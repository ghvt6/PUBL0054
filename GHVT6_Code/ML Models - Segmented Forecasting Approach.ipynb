{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing and Managing a Cached Function for Optimised Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_function(args):\n",
    "    # This function is decorated with @lru_cache, which enables caching of its results.\n",
    "    # The 'maxsize' parameter specifies that up to 128 results can be stored in the cache.\n",
    "    # Caching can significantly improve performance by avoiding redundant calculations for repeated inputs.\n",
    "    # When the function is called with the same arguments, the cached result is returned instead of recomputing it.\n",
    "    # The function implementation should be inserted here.\n",
    "    pass\n",
    "\n",
    "# The cache associated with the 'cached_function' is cleared.\n",
    "# This is useful in scenarios where the cached results are no longer valid or when it is necessary to free up memory.\n",
    "cached_function.cache_clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of segmented approach for forecasting asylum applications using ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, median_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Attention, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Set the font to Times New Roman globally for all plots\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Ensure reproducibility with TensorFlow 2.x by setting deterministic operations\n",
    "tf.keras.utils.set_random_seed(random_seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Create an output directory for graphs and other results\n",
    "output_dir = \"Segmented Approach Outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"final_thesis_data.csv\")\n",
    "data['year_month'] = pd.to_datetime(data['year_month'])\n",
    "data = data.sort_values(by=['country', 'year_month'])\n",
    "\n",
    "# Aggregate data by country, year_month, sex, and age_group\n",
    "data_agg = data.groupby(['country', 'year_month', 'sex', 'age_group']).sum().reset_index()\n",
    "\n",
    "# Define variables to lag and create lagged variables\n",
    "variables_to_lag = {\n",
    "    \"illegal_border_crossings\": [3],\n",
    "    \"push_factor_index\": [3],\n",
    "    \"push_factor_index_high_level\": [2],\n",
    "    \"deaths_civilians\": [12],\n",
    "    \"gdp_per_capita_current_usd\": [12],\n",
    "    \"gdp_per_capita_growth\": [12],\n",
    "    \"regime_end_type\": [0],\n",
    "    \"state_fiscal_source_revenue\": [0],\n",
    "    \"state_authority_over_territory\": [0],\n",
    "    \"political_polarisation\": [0],\n",
    "    \"political_violence\": [0],\n",
    "    \"domestic_autonomy\": [0],\n",
    "    \"rule_of_law\": [0],\n",
    "}\n",
    "\n",
    "# Create lagged variables for the predictors\n",
    "for var, lags in variables_to_lag.items():\n",
    "    for lag in lags:\n",
    "        lagged_var_name = f\"{var}_lag_{lag}\"\n",
    "        data_agg[lagged_var_name] = data_agg.groupby(['country', 'sex', 'age_group'])[var].shift(lag)\n",
    "\n",
    "# Define rolling window sizes\n",
    "rolling_windows = [3, 6, 12, 24]\n",
    "\n",
    "# Calculate rolling mean and standard deviation for asylum applications with different window sizes\n",
    "for window in rolling_windows:\n",
    "    data_agg[f'asy_applications_rolling_mean_{window}'] = data_agg.groupby(['country', 'sex', 'age_group'])['asy_applications'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    data_agg[f'asy_applications_rolling_sd_{window}'] = data_agg.groupby(['country', 'sex', 'age_group'])['asy_applications'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "\n",
    "# Calculate rolling mean and standard deviation for push_factor_index with different window sizes\n",
    "for window in rolling_windows:\n",
    "    data_agg[f'push_factor_index_rolling_mean_{window}'] = data_agg.groupby(['country', 'sex', 'age_group'])['push_factor_index'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    data_agg[f'push_factor_index_rolling_sd_{window}'] = data_agg.groupby(['country', 'sex', 'age_group'])['push_factor_index'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "\n",
    "# Calculate exponential moving average for asylum applications\n",
    "data_agg['asy_applications_ewm_mean'] = data_agg.groupby(['country', 'sex', 'age_group'])['asy_applications'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "data_agg['asy_applications_ewm_std'] = data_agg.groupby(['country', 'sex', 'age_group'])['asy_applications'].transform(lambda x: x.ewm(span=12, adjust=False).std())\n",
    "\n",
    "# Add temporal features for seasonality\n",
    "data_agg['month'] = data_agg['year_month'].dt.month\n",
    "data_agg['month_sin'] = np.sin(2 * np.pi * data_agg['month'] / 12)\n",
    "data_agg['month_cos'] = np.cos(2 * np.pi * data_agg['month'] / 12)\n",
    "\n",
    "# Define the predictors and the target variable\n",
    "predictors = [\n",
    "    \"illegal_border_crossings_lag_3\", \"push_factor_index_lag_3\",\n",
    "    \"push_factor_index_high_level_lag_2\", \"deaths_civilians_lag_12\",\n",
    "    \"gdp_per_capita_current_usd_lag_12\", \"gdp_per_capita_growth_lag_12\",\n",
    "    \"regime_end_type_lag_0\", \"state_fiscal_source_revenue_lag_0\",\n",
    "    \"state_authority_over_territory_lag_0\", \"political_polarisation_lag_0\",\n",
    "    \"political_violence_lag_0\", \"domestic_autonomy_lag_0\",\n",
    "    \"rule_of_law_lag_0\",\n",
    "    \"asy_applications_rolling_mean_3\", \"asy_applications_rolling_sd_3\",\n",
    "    \"asy_applications_rolling_mean_6\", \"asy_applications_rolling_sd_6\",\n",
    "    \"asy_applications_rolling_mean_12\", \"asy_applications_rolling_sd_12\",\n",
    "    \"asy_applications_rolling_mean_24\", \"asy_applications_rolling_sd_24\",\n",
    "    \"push_factor_index_rolling_mean_3\", \"push_factor_index_rolling_sd_3\",\n",
    "    \"push_factor_index_rolling_mean_6\", \"push_factor_index_rolling_sd_6\",\n",
    "    \"push_factor_index_rolling_mean_12\", \"push_factor_index_rolling_sd_12\",\n",
    "    \"push_factor_index_rolling_mean_24\", \"push_factor_index_rolling_sd_24\",\n",
    "    \"asy_applications_ewm_mean\", \"asy_applications_ewm_std\",\n",
    "    \"month_sin\", \"month_cos\"\n",
    "]\n",
    "target_var = \"asy_applications\"\n",
    "\n",
    "# Handle missing values using Iterative Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=random_seed)\n",
    "data_agg[predictors] = imputer.fit_transform(data_agg[predictors])\n",
    "\n",
    "# Scale the predictors and target variable using RobustScaler\n",
    "scaler_x = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "data_agg[predictors] = scaler_x.fit_transform(data_agg[predictors])\n",
    "data_agg[target_var] = scaler_y.fit_transform(data_agg[target_var].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define a DataFrame to store residuals data for all months\n",
    "all_residuals = pd.DataFrame()\n",
    "\n",
    "# Initialise storage for variance scores for each model\n",
    "variance_scores = {\n",
    "    'XGBoost': [],\n",
    "    'LSTM': [],\n",
    "    'NN': []\n",
    "}\n",
    "\n",
    "# Initialise a list to store DataFrames of performance metrics for each group\n",
    "metrics_per_group_list = []\n",
    "\n",
    "# Function to create and train the LSTM model with L2 regularisation, dropout rate, and attention mechanism\n",
    "def create_and_train_lstm_with_attention(X_train, y_train, X_val, y_val, units_1=32, units_2=32, dropout_rate=0.5, learning_rate=0.001, l2_lambda=0.01):\n",
    "    # Define the input layer for the model\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Define LSTM layers with dropout and L2 regularisation\n",
    "    lstm_out = LSTM(units_1, return_sequences=True, kernel_regularizer=l2(l2_lambda))(inputs)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    lstm_out = LSTM(units_2, return_sequences=True, kernel_regularizer=l2(l2_lambda))(lstm_out)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    # Add an attention layer\n",
    "    attention_out = Attention()([lstm_out, lstm_out])\n",
    "    attention_out = Dense(units_2, activation='relu')(attention_out)\n",
    "    attention_out = Flatten()(attention_out)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    outputs = Dense(1, kernel_regularizer=l2(l2_lambda), activation='linear')(attention_out)\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    # Add early stopping based on validation loss\n",
    "    early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping_lstm], verbose=0)\n",
    "    \n",
    "    # Calculate explained variance on validation data\n",
    "    val_pred = model.predict(X_val).flatten()\n",
    "    val_explained_variance = explained_variance_score(y_val, val_pred)\n",
    "    variance_scores['LSTM'].append(val_explained_variance)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to create and train a neural network model with optimal hyperparameters, L2 regularisation, and dropout\n",
    "def create_best_nn_model(X_train, y_train, X_val, y_val, dropout_rate=0.3, units_1=128, units_2=64, learning_rate=0.001, l2_lambda=0.01):\n",
    "    # Define the sequential model\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),  # Ensure this is the first layer\n",
    "        Dense(units_1, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units_2, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, kernel_regularizer=l2(l2_lambda), activation='linear')  # Use linear activation\n",
    "    ])\n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    early_stopping_nn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping_nn], verbose=0)\n",
    "    \n",
    "    # Calculate explained variance on validation data\n",
    "    val_pred = model.predict(X_val).flatten()\n",
    "    val_explained_variance = explained_variance_score(y_val, val_pred)\n",
    "    variance_scores['NN'].append(val_explained_variance)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to train an XGBoost model with optimal hyperparameters\n",
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    # Define the XGBoost model with the specified hyperparameters\n",
    "    model = xgb.XGBRegressor(\n",
    "        learning_rate=0.2,\n",
    "        max_depth=7,\n",
    "        n_estimators=100,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=random_seed,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # Calculate explained variance on validation data\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_explained_variance = explained_variance_score(y_val, val_pred)\n",
    "    variance_scores['XGBoost'].append(val_explained_variance)\n",
    "    \n",
    "    return model, val_explained_variance  # Return the model and variance score for consistency\n",
    "\n",
    "# Function to split data for training and validation based on quarters\n",
    "def split_train_val(data, quarter='Q1'):\n",
    "    # Convert dates to quarters\n",
    "    data['quarter'] = data['year_month'].dt.to_period('Q')\n",
    "    val_data = data[data['quarter'].astype(str).str.endswith(quarter)]\n",
    "    train_data = data[~data.index.isin(val_data.index)]\n",
    "    return train_data, val_data\n",
    "\n",
    "# Function to calculate confidence intervals for predictions\n",
    "def calculate_confidence_intervals(predictions, confidence_level=0.95):\n",
    "    ## Calculate the confidence intervals for the predictions ## \n",
    "    mean_preds = np.mean(predictions, axis=0)\n",
    "    std_dev = np.std(predictions, axis=0)\n",
    "    z_score = 1.96  # Corresponds to 95% confidence interval\n",
    "    lower_bound = mean_preds - z_score * std_dev\n",
    "    upper_bound = mean_preds + z_score * std_dev\n",
    "    return mean_preds, lower_bound, upper_bound\n",
    "\n",
    "# Function for bootstrapping predictions for a model\n",
    "def bootstrap_predictions(model_func, X_train, y_train, X_val, y_val, X_test, n_iterations=20):\n",
    "    predictions = []\n",
    "    for _ in range(n_iterations):\n",
    "        train_indices = np.random.choice(range(len(X_train)), size=len(X_train), replace=True)\n",
    "        X_train_resample = X_train[train_indices]\n",
    "        y_train_resample = y_train[train_indices]\n",
    "\n",
    "        # Check for empty arrays\n",
    "        if X_train_resample.size == 0 or y_train_resample.size == 0:\n",
    "            continue  # Skip this iteration if data is empty\n",
    "\n",
    "        # Reshape for LSTM if the function name matches\n",
    "        if model_func.__name__ == 'create_and_train_lstm_with_attention':\n",
    "            X_train_resample = np.reshape(X_train_resample, (X_train_resample.shape[0], 1, X_train_resample.shape[1]))\n",
    "            X_val_resample = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "            X_test_resample = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "        else:\n",
    "            X_val_resample = X_val  # No reshaping needed for non-LSTM models\n",
    "            X_test_resample = X_test\n",
    "\n",
    "        # Train the model\n",
    "        model, _ = model_func(X_train_resample, y_train_resample, X_val_resample, y_val)\n",
    "\n",
    "        # Predict on test data\n",
    "        preds = model.predict(X_test_resample).flatten()\n",
    "        predictions.append(preds)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Function to compute weights inversely proportional to RMSE\n",
    "def calculate_weights(rmse_values):\n",
    "    ## Calculate weights inversely proportional to the RMSE ##\n",
    "    inv_rmse = 1 / np.array(rmse_values)\n",
    "    return inv_rmse / np.sum(inv_rmse)\n",
    "\n",
    "# Function to plot combined residuals over time\n",
    "def plot_combined_residuals(all_residuals):\n",
    "    countries = all_residuals['country'].unique()\n",
    "    for country in countries:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        sex_age_groups = all_residuals[all_residuals['country'] == country].groupby(['sex', 'age_group'])\n",
    "\n",
    "        for i, ((sex, age_group), _) in enumerate(sex_age_groups, start=1):\n",
    "            plt.subplot(2, 2, i)\n",
    "            # Creating filters specific for each DataFrame's index\n",
    "            group_filter = (all_residuals['country'] == country) & (all_residuals['sex'] == sex) & (all_residuals['age_group'] == age_group)\n",
    "            group_data = all_residuals.loc[group_filter]\n",
    "\n",
    "            # Plot residuals for each dataset with different markers and colours\n",
    "            train_data = group_data[group_data['data_split'] == 'train']\n",
    "            val_data = group_data[group_data['data_split'] == 'val']\n",
    "            test_data = group_data[group_data['data_split'] == 'test']\n",
    "\n",
    "            plt.scatter(train_data['year_month'], train_data['Residuals'], alpha=0.5, label='Train', marker='o', color='blue')\n",
    "            plt.scatter(val_data['year_month'], val_data['Residuals'], alpha=0.5, label='Validation', marker='x', color='green')\n",
    "            plt.scatter(test_data['year_month'], test_data['Residuals'], alpha=0.5, label='Test', marker='^', color='red')\n",
    "\n",
    "            plt.title(f'Residuals - {country}, {sex}, {age_group}')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.axhline(0, color='red', linestyle='--')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/Residuals_{country}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Function to plot average feature importances for each month\n",
    "def plot_aggregated_feature_importances(xgboost_models, predictors, output_dir, month):\n",
    "    # Initialise a list to hold the aggregated importances\n",
    "    aggregated_importances = np.zeros(len(predictors))\n",
    "    \n",
    "    # Compute the mean importances for this month's models\n",
    "    importances = np.array([model.feature_importances_ for model in xgboost_models])\n",
    "    mean_importances = np.mean(importances, axis=0)\n",
    "    \n",
    "    # Sorting indices for plotting (highest to lowest)\n",
    "    sorted_indices = np.argsort(mean_importances)[::-1]\n",
    "    \n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(np.array(predictors)[sorted_indices], mean_importances[sorted_indices], color='skyblue')\n",
    "    plt.xlabel('Average Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Average Feature Importance for {month}')\n",
    "    plt.gca().invert_yaxis()  # Show most important feature at the top\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to specified directory\n",
    "    plt.savefig(f'{output_dir}/Aggregated_Feature_Importances_{month}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Function to calculate explained variance at the aggregated level per month\n",
    "def calculate_aggregated_explained_variance_per_month(results_df, prediction_column):\n",
    "    # Aggregate by country and month\n",
    "    aggregated_df = results_df.groupby(['country', 'year_month']).sum(numeric_only=True).reset_index()\n",
    "\n",
    "    # Calculate explained variance\n",
    "    explained_variance = explained_variance_score(aggregated_df['True Values'], aggregated_df[prediction_column])\n",
    "\n",
    "    return pd.DataFrame({'Date': [results_df['year_month'].iloc[0]], 'Explained Variance': [explained_variance]})\n",
    "\n",
    "# Initialise DataFrames to store explained variance for each model per month\n",
    "explained_variance_lstm_df = pd.DataFrame(columns=['Date', 'Explained Variance'])\n",
    "explained_variance_nn_df = pd.DataFrame(columns=['Date', 'Explained Variance'])\n",
    "explained_variance_xgb_df = pd.DataFrame(columns=['Date', 'Explained Variance'])\n",
    "explained_variance_ensemble_df = pd.DataFrame(columns=['Date', 'Explained Variance'])\n",
    "\n",
    "# Initialise an empty DataFrame to store results for all months\n",
    "all_results_df = pd.DataFrame()\n",
    "\n",
    "# Initialise an empty DataFrame to store the explained variance for all dates\n",
    "explained_variance_df = pd.DataFrame(columns=['Date', 'Explained Variance XGBoost', 'Explained Variance LSTM', 'Explained Variance NN', 'Explained Variance Ensemble'])\n",
    "\n",
    "# Define prediction dates\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']\n",
    "\n",
    "# TimeSeriesSplit object for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Loop through each prediction date and generate forecasts\n",
    "for date in prediction_dates:\n",
    "    print(f\"Processing predictions for {date}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_data, val_data = split_train_val(data_agg, quarter='Q4')\n",
    "    test_data = data_agg[data_agg['year_month'] == date]\n",
    "    \n",
    "    X_train = train_data[predictors]\n",
    "    y_train = train_data[target_var]\n",
    "    X_val = val_data[predictors]\n",
    "    y_val = val_data[target_var]\n",
    "    X_test = test_data[predictors]\n",
    "    y_test = test_data[target_var]\n",
    "    \n",
    "    # Perform walk-forward validation\n",
    "    lstm_history = []\n",
    "    nn_history = []\n",
    "    xgboost_models = []\n",
    "\n",
    "    # Convert to NumPy arrays for model input\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    # Initialise lists to store RMSE values\n",
    "    lstm_rmse_train = []\n",
    "    nn_rmse_train = []\n",
    "    xgboost_rmse_train = []\n",
    "\n",
    "    lstm_rmse_val = []\n",
    "    nn_rmse_val = []\n",
    "    xgboost_rmse_val = []\n",
    "\n",
    "    # Perform time-series cross-validation\n",
    "    for train_index, val_index in tscv.split(X_train_np):\n",
    "        X_train_cv, X_val_cv = X_train_np[train_index], X_train_np[val_index]\n",
    "        y_train_cv, y_val_cv = y_train_np[train_index], y_train_np[val_index]\n",
    "        \n",
    "        X_train_lstm = np.reshape(X_train_cv, (X_train_cv.shape[0], 1, X_train_cv.shape[1]))\n",
    "        X_val_lstm = np.reshape(X_val_cv, (X_val_cv.shape[0], 1, X_val_cv.shape[1]))\n",
    "        \n",
    "        # Number of models to train for ensemble\n",
    "        num_models = 5\n",
    "        \n",
    "        lstm_models = []\n",
    "        for i in range(num_models):\n",
    "            # Set a new random seed for each model based on the loop index\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "            # Train LSTM model\n",
    "            model, history = create_and_train_lstm_with_attention(X_train_lstm, y_train_cv, X_val_lstm, y_val_cv)\n",
    "            lstm_models.append(model)\n",
    "            lstm_history.append(history)\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_lstm = model.predict(X_train_lstm).flatten()\n",
    "            val_pred_lstm = model.predict(X_val_lstm).flatten()\n",
    "\n",
    "            lstm_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_lstm)))\n",
    "            lstm_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_lstm)))\n",
    "        \n",
    "        nn_models = []\n",
    "        for i in range(num_models):\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            \n",
    "            # Train NN model\n",
    "            model, history = create_best_nn_model(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "            nn_models.append(model)\n",
    "            nn_history.append(history)\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_nn = model.predict(X_train_cv).flatten()\n",
    "            val_pred_nn = model.predict(X_val_cv).flatten()\n",
    "\n",
    "            nn_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_nn)))\n",
    "            nn_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_nn)))\n",
    "\n",
    "        xgboost_models_this_month = []\n",
    "        for i in range(num_models):\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            \n",
    "            # Train XGBoost model\n",
    "            model, _ = train_xgboost_model(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "            xgboost_models_this_month.append(model)  # Append only the model\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_xgb = model.predict(X_train_cv)\n",
    "            val_pred_xgb = model.predict(X_val_cv)\n",
    "\n",
    "            xgboost_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_xgb)))\n",
    "            xgboost_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_xgb)))\n",
    "\n",
    "        xgboost_models.append(xgboost_models_this_month)\n",
    "\n",
    "    # Calculate average RMSE over all models for training and validation\n",
    "    avg_lstm_rmse_train = np.mean(lstm_rmse_train)\n",
    "    avg_nn_rmse_train = np.mean(nn_rmse_train)\n",
    "    avg_xgboost_rmse_train = np.mean(xgboost_rmse_train)\n",
    "\n",
    "    avg_lstm_rmse_val = np.mean(lstm_rmse_val)\n",
    "    avg_nn_rmse_val = np.mean(nn_rmse_val)\n",
    "    avg_xgboost_rmse_val = np.mean(xgboost_rmse_val)\n",
    "\n",
    "    # Calculate weights based on the average RMSE\n",
    "    train_weights = calculate_weights([avg_xgboost_rmse_train, avg_lstm_rmse_train, avg_nn_rmse_train])\n",
    "    val_weights = calculate_weights([avg_xgboost_rmse_val, avg_lstm_rmse_val, avg_nn_rmse_val])\n",
    "\n",
    "    # Generate bootstrapped predictions for each model\n",
    "    lstm_bootstrap_preds = bootstrap_predictions(\n",
    "        create_and_train_lstm_with_attention, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "    nn_bootstrap_preds = bootstrap_predictions(\n",
    "        create_best_nn_model, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "    xgboost_bootstrap_preds = bootstrap_predictions(\n",
    "        train_xgboost_model, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "\n",
    "    # Calculate mean and confidence intervals for each model's predictions\n",
    "    lstm_mean, lstm_lower_ci, lstm_upper_ci = calculate_confidence_intervals(lstm_bootstrap_preds)\n",
    "    nn_mean, nn_lower_ci, nn_upper_ci = calculate_confidence_intervals(nn_bootstrap_preds)\n",
    "    xgboost_mean, xgboost_lower_ci, xgboost_upper_ci = calculate_confidence_intervals(xgboost_bootstrap_preds)\n",
    "\n",
    "    # Calculate CI widths and choose the model with the widest CI for ensemble predictions\n",
    "    lstm_ci_width = lstm_upper_ci - lstm_lower_ci\n",
    "    nn_ci_width = nn_upper_ci - nn_lower_ci\n",
    "    xgboost_ci_width = xgboost_upper_ci - xgboost_lower_ci\n",
    "\n",
    "    # Create an array to hold ensemble predictions using the model with the widest CI\n",
    "    ensemble_mean_widest_ci = np.zeros_like(lstm_mean)\n",
    "    ensemble_lower_ci = np.zeros_like(lstm_lower_ci)  # Define these arrays to store ensemble CI data\n",
    "    ensemble_upper_ci = np.zeros_like(lstm_upper_ci)\n",
    "\n",
    "    # Selecting the model with the widest CI for each instance\n",
    "    for i in range(len(ensemble_mean_widest_ci)):\n",
    "        if lstm_ci_width[i] >= nn_ci_width[i] and lstm_ci_width[i] >= xgboost_ci_width[i]:\n",
    "            ensemble_mean_widest_ci[i] = lstm_mean[i]\n",
    "            ensemble_lower_ci[i] = lstm_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = lstm_upper_ci[i]\n",
    "        elif nn_ci_width[i] >= lstm_ci_width[i] and nn_ci_width[i] >= xgboost_ci_width[i]:\n",
    "            ensemble_mean_widest_ci[i] = nn_mean[i]\n",
    "            ensemble_lower_ci[i] = nn_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = nn_upper_ci[i]\n",
    "        else:\n",
    "            ensemble_mean_widest_ci[i] = xgboost_mean[i]\n",
    "            ensemble_lower_ci[i] = xgboost_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = xgboost_upper_ci[i]\n",
    "\n",
    "    # Rescale predictions back to original scale\n",
    "    y_test_rescaled = scaler_y.inverse_transform(y_test_np.reshape(-1, 1)).flatten()\n",
    "    ensemble_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_mean_widest_ci.reshape(-1, 1)).flatten())\n",
    "    \n",
    "    results_df = test_data[['country', 'sex', 'age_group', 'year_month']].copy()\n",
    "    results_df['True Values'] = np.round(y_test_rescaled[:results_df.shape[0]])\n",
    "    results_df['Ensemble Prediction'] = np.round(ensemble_pred_rescaled[:results_df.shape[0]])\n",
    "\n",
    "    # Add lower and upper CIs for each model and the ensemble\n",
    "    results_df['XGBoost Lower CI'] = np.round(scaler_y.inverse_transform(xgboost_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['XGBoost Upper CI'] = np.round(scaler_y.inverse_transform(xgboost_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['LSTM Lower CI'] = np.round(scaler_y.inverse_transform(lstm_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['LSTM Upper CI'] = np.round(scaler_y.inverse_transform(lstm_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['NN Lower CI'] = np.round(scaler_y.inverse_transform(nn_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['NN Upper CI'] = np.round(scaler_y.inverse_transform(nn_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    \n",
    "    results_df['Ensemble Lower CI'] = np.round(scaler_y.inverse_transform(ensemble_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['Ensemble Upper CI'] = np.round(scaler_y.inverse_transform(ensemble_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['Residuals'] = results_df['True Values'] - results_df['Ensemble Prediction']\n",
    "\n",
    "    # Store individual model predictions\n",
    "    results_df['XGBoost Prediction'] = np.round(scaler_y.inverse_transform(xgboost_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['LSTM Prediction'] = np.round(scaler_y.inverse_transform(lstm_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['NN Prediction'] = np.round(scaler_y.inverse_transform(nn_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    \n",
    "    # Calculate residuals for training and validation sets\n",
    "    train_pred_xgb_list = [model.predict(X_train_np) for model in xgboost_models_this_month]\n",
    "    train_pred_xgb = np.mean(train_pred_xgb_list, axis=0)\n",
    "    \n",
    "    train_pred_lstm_list = [model.predict(np.reshape(X_train_np, (X_train_np.shape[0], 1, X_train_np.shape[1]))).flatten() for model in lstm_models]\n",
    "    train_pred_lstm = np.mean(train_pred_lstm_list, axis=0)\n",
    "    \n",
    "    train_pred_nn_list = [model.predict(X_train_np).flatten() for model in nn_models]\n",
    "    train_pred_nn = np.mean(train_pred_nn_list, axis=0)\n",
    "\n",
    "    # Compute weighted average for train predictions\n",
    "    ensemble_train_pred = (train_weights[0] * train_pred_xgb +\n",
    "                           train_weights[1] * train_pred_lstm +\n",
    "                           train_weights[2] * train_pred_nn)\n",
    "\n",
    "    # Create train meta-features without confidence intervals\n",
    "    train_meta_features = pd.DataFrame({\n",
    "        'XGBoost': train_pred_xgb,\n",
    "        'LSTM': train_pred_lstm,\n",
    "        'NN': train_pred_nn\n",
    "    }, dtype='float32')  # Ensure inputs are floats\n",
    "    \n",
    "    train_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_train_pred.reshape(-1, 1)).flatten())\n",
    "    y_train_rescaled = scaler_y.inverse_transform(y_train_np.reshape(-1, 1)).flatten()\n",
    "    train_residuals = y_train_rescaled - train_pred_rescaled\n",
    "    train_data['Residuals'] = train_residuals\n",
    "\n",
    "    val_pred_xgb_list = [model.predict(X_val_np) for model in xgboost_models_this_month]\n",
    "    val_pred_xgb = np.mean(val_pred_xgb_list, axis=0)\n",
    "\n",
    "    val_pred_lstm_list = [model.predict(np.reshape(X_val_np, (X_val_np.shape[0], 1, X_val_np.shape[1]))).flatten() for model in lstm_models]\n",
    "    val_pred_lstm = np.mean(val_pred_lstm_list, axis=0)\n",
    "    \n",
    "    val_pred_nn_list = [model.predict(X_val_np).flatten() for model in nn_models]\n",
    "    val_pred_nn = np.mean(val_pred_nn_list, axis=0)\n",
    "\n",
    "    # Compute weighted average for validation predictions\n",
    "    ensemble_val_pred = (val_weights[0] * val_pred_xgb +\n",
    "                         val_weights[1] * val_pred_lstm +\n",
    "                         val_weights[2] * val_pred_nn)\n",
    "\n",
    "    val_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_val_pred.reshape(-1, 1)).flatten())\n",
    "    y_val_rescaled = scaler_y.inverse_transform(y_val_np.reshape(-1, 1)).flatten()\n",
    "    val_residuals = y_val_rescaled - val_pred_rescaled\n",
    "    val_data['Residuals'] = val_residuals\n",
    "\n",
    "    # Append residuals to the all_residuals DataFrame\n",
    "    test_residuals = results_df['True Values'] - results_df['Ensemble Prediction']\n",
    "    test_data['Residuals'] = test_residuals\n",
    "    \n",
    "    # Add a column to identify data splits\n",
    "    train_data['data_split'] = 'train'\n",
    "    val_data['data_split'] = 'val'\n",
    "    test_data['data_split'] = 'test'\n",
    "    \n",
    "    all_residuals = pd.concat([\n",
    "        all_residuals,\n",
    "        train_data[['country', 'sex', 'age_group', 'year_month', 'Residuals', 'data_split']],\n",
    "        val_data[['country', 'sex', 'age_group', 'year_month', 'Residuals', 'data_split']],\n",
    "        test_data[['country', 'sex', 'age_group', 'year_month', 'Residuals', 'data_split']]\n",
    "    ])\n",
    "\n",
    "\n",
    "    groups = results_df.groupby(['country', 'sex', 'age_group'])\n",
    "    for (country, sex, age_group), group_data in groups:\n",
    "        print(f\"Residuals for Country: {country}, Sex: {sex}, Age Group: {age_group}\")\n",
    "        print(group_data[['year_month', 'Residuals']])\n",
    "    \n",
    "    results_df.to_csv(f'{output_dir}/forecasts_with_uncertainty_group_level_{date}.csv', index=False)\n",
    "    \n",
    "    metrics_per_group = []\n",
    "    groups = results_df.groupby(['country', 'sex', 'age_group'])\n",
    "\n",
    "    for (country, sex, age_group), group_data in groups:\n",
    "        y_true_group = group_data['True Values'].values\n",
    "        y_pred_group = group_data['Ensemble Prediction'].values\n",
    "        y_pred_xgb_group = group_data['XGBoost Prediction'].values\n",
    "        y_pred_lstm_group = group_data['LSTM Prediction'].values\n",
    "        y_pred_nn_group = group_data['NN Prediction'].values\n",
    "        \n",
    "        if not np.any(np.isnan(y_true_group)) and not np.any(np.isnan(y_pred_group)):\n",
    "            mse_group = mean_squared_error(y_true_group, y_pred_group)\n",
    "            rmse_group = np.sqrt(mse_group)  \n",
    "            mae_group = mean_absolute_error(y_true_group, y_pred_group)\n",
    "            mdae_group = median_absolute_error(y_true_group, y_pred_group)\n",
    "            mape_group = mean_absolute_percentage_error(y_true_group + 1e-6, y_pred_group)\n",
    "\n",
    "            mse_xgb_group = mean_squared_error(y_true_group, y_pred_xgb_group)\n",
    "            rmse_xgb_group = np.sqrt(mse_xgb_group)\n",
    "            mae_xgb_group = mean_absolute_error(y_true_group, y_pred_xgb_group)\n",
    "            mdae_xgb_group = median_absolute_error(y_true_group, y_pred_xgb_group)\n",
    "            mape_xgb_group = mean_absolute_percentage_error(y_true_group + 1e-6, y_pred_xgb_group)\n",
    "\n",
    "            mse_lstm_group = mean_squared_error(y_true_group, y_pred_lstm_group)\n",
    "            rmse_lstm_group = np.sqrt(mse_lstm_group)\n",
    "            mae_lstm_group = mean_absolute_error(y_true_group, y_pred_lstm_group)\n",
    "            mdae_lstm_group = median_absolute_error(y_true_group, y_pred_lstm_group)\n",
    "            mape_lstm_group = mean_absolute_percentage_error(y_true_group + 1e-6, y_pred_lstm_group)\n",
    "\n",
    "            mse_nn_group = mean_squared_error(y_true_group, y_pred_nn_group)\n",
    "            rmse_nn_group = np.sqrt(mse_nn_group)\n",
    "            mae_nn_group = mean_absolute_error(y_true_group, y_pred_nn_group)\n",
    "            mdae_nn_group = median_absolute_error(y_true_group, y_pred_nn_group)\n",
    "            mape_nn_group = mean_absolute_percentage_error(y_true_group + 1e-6, y_pred_nn_group)\n",
    "            \n",
    "            metrics_per_group.append({\n",
    "                'Country': country,\n",
    "                'Sex': sex,\n",
    "                'Age Group': age_group,\n",
    "                'MSE Ensemble': np.round(mse_group),\n",
    "                'RMSE Ensemble': np.round(rmse_group),\n",
    "                'MAE Ensemble': np.round(mae_group),\n",
    "                'MDAE Ensemble': np.round(mdae_group),\n",
    "                'MAPE Ensemble': np.round(mape_group),\n",
    "                'MSE XGBoost': np.round(mse_xgb_group),\n",
    "                'RMSE XGBoost': np.round(rmse_xgb_group),\n",
    "                'MAE XGBoost': np.round(mae_xgb_group),\n",
    "                'MDAE XGBoost': np.round(mdae_xgb_group),\n",
    "                'MAPE XGBoost': np.round(mape_xgb_group),\n",
    "                'MSE LSTM': np.round(mse_lstm_group),\n",
    "                'RMSE LSTM': np.round(rmse_lstm_group),\n",
    "                'MAE LSTM': np.round(mae_lstm_group),\n",
    "                'MDAE LSTM': np.round(mdae_lstm_group),\n",
    "                'MAPE LSTM': np.round(mape_lstm_group),\n",
    "                'MSE NN': np.round(mse_nn_group),\n",
    "                'RMSE NN': np.round(rmse_nn_group),\n",
    "                'MAE NN': np.round(mae_nn_group),\n",
    "                'MDAE NN': np.round(mdae_nn_group),\n",
    "                'MAPE NN': np.round(mape_nn_group)\n",
    "            })\n",
    "\n",
    "    metrics_group_df = pd.DataFrame(metrics_per_group)\n",
    "    metrics_per_group_list.append(metrics_group_df)\n",
    "\n",
    "    metrics_group_df.to_csv(f'{output_dir}/performance_metrics_per_group_{date}.csv', index=False)\n",
    "\n",
    "    # Aggregate to country level by summing up the metrics\n",
    "    metrics_per_country = metrics_group_df.groupby('Country').sum(numeric_only=True).reset_index()\n",
    "    metrics_per_country.to_csv(f'{output_dir}/performance_metrics_per_country_{date}.csv', index=False)\n",
    "\n",
    "    # Aggregate to total by summing up the country-level metrics\n",
    "    overall_metrics = metrics_per_country.sum(numeric_only=True).to_dict()\n",
    "    overall_metrics_df = pd.DataFrame([overall_metrics])\n",
    "    overall_metrics_df.to_csv(f'{output_dir}/performance_metrics_overall_{date}.csv', index=False)\n",
    "\n",
    "    # Plot feature importances for each month\n",
    "    plot_aggregated_feature_importances(xgboost_models_this_month, predictors, output_dir, date)\n",
    "\n",
    "    # Calculate explained variance for XGBoost, LSTM, NN, and Ensemble\n",
    "    explained_variance_xgb = explained_variance_score(y_test_rescaled, xgboost_mean)\n",
    "    explained_variance_lstm = explained_variance_score(y_test_rescaled, lstm_mean)\n",
    "    explained_variance_nn = explained_variance_score(y_test_rescaled, nn_mean)\n",
    "    explained_variance_ensemble = explained_variance_score(y_test_rescaled, ensemble_pred_rescaled)\n",
    "\n",
    "    # Add explained variance to DataFrame\n",
    "    explained_variance_df = pd.concat([explained_variance_df, pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'Explained Variance XGBoost': [explained_variance_xgb],\n",
    "        'Explained Variance LSTM': [explained_variance_lstm],\n",
    "        'Explained Variance NN': [explained_variance_nn],\n",
    "        'Explained Variance Ensemble': [explained_variance_ensemble]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    print(f\"Explained Variance - XGBoost: {explained_variance_xgb:.4f}\")\n",
    "    print(f\"Explained Variance - LSTM: {explained_variance_lstm:.4f}\")\n",
    "    print(f\"Explained Variance - NN: {explained_variance_nn:.4f}\")\n",
    "    print(f\"Explained Variance - Ensemble: {explained_variance_ensemble:.4f}\")\n",
    "\n",
    "    # Calculate and store explained variance at the aggregated level per month\n",
    "    explained_variance_lstm_df = pd.concat([explained_variance_lstm_df, calculate_aggregated_explained_variance_per_month(results_df, 'LSTM Prediction')])\n",
    "    explained_variance_nn_df = pd.concat([explained_variance_nn_df, calculate_aggregated_explained_variance_per_month(results_df, 'NN Prediction')])\n",
    "    explained_variance_xgb_df = pd.concat([explained_variance_xgb_df, calculate_aggregated_explained_variance_per_month(results_df, 'XGBoost Prediction')])\n",
    "    explained_variance_ensemble_df = pd.concat([explained_variance_ensemble_df, calculate_aggregated_explained_variance_per_month(results_df, 'Ensemble Prediction')])\n",
    "\n",
    "    # Aggregate true values and predicted values at the country and month level\n",
    "    country_month_aggregation = results_df.groupby(['country', 'year_month']).sum(numeric_only=True).reset_index()\n",
    "    country_month_aggregation.to_csv(f'{output_dir}/country_level_aggregation_{date}.csv', index=False)\n",
    "\n",
    "    # Aggregate true values and predicted values overall by summing up the country-month-level metrics\n",
    "    overall_aggregation = country_month_aggregation.sum(numeric_only=True).to_dict()\n",
    "    overall_aggregation_df = pd.DataFrame([overall_aggregation])\n",
    "    overall_aggregation_df.to_csv(f'{output_dir}/overall_aggregation_{date}.csv', index=False)\n",
    "\n",
    "# Save explained variance for each model per month\n",
    "explained_variance_df.to_csv(f'{output_dir}/explained_variance_by_model_per_month.csv', index=False)\n",
    "\n",
    "# Save the aggregated explained variance per month to CSV files\n",
    "explained_variance_lstm_df.to_csv(f'{output_dir}/explained_variance_per_month_LSTM.csv', index=False)\n",
    "explained_variance_nn_df.to_csv(f'{output_dir}/explained_variance_per_month_NN.csv', index=False)\n",
    "explained_variance_xgb_df.to_csv(f'{output_dir}/explained_variance_per_month_XGBoost.csv', index=False)\n",
    "explained_variance_ensemble_df.to_csv(f'{output_dir}/explained_variance_per_month_Ensemble.csv', index=False)\n",
    "\n",
    "# Plot combined residuals over time for each group after all forecasts\n",
    "plot_combined_residuals(all_residuals)\n",
    "\n",
    "# Plot learning curves for LSTM models\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, history in enumerate(lstm_history):\n",
    "    plt.plot(history.history['loss'], label=f'LSTM Train {i+1}', linestyle='-')\n",
    "    plt.plot(history.history['val_loss'], label=f'LSTM Val {i+1}', linestyle='--')\n",
    "plt.title('Learning Curves for LSTM Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/LSTM_Learning_Curves.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot learning curves for NN models\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, history in enumerate(nn_history):\n",
    "    plt.plot(history.history['loss'], label=f'NN Train {i+1}', linestyle='-')\n",
    "    plt.plot(history.history['val_loss'], label=f'NN Val {i+1}', linestyle='--')\n",
    "plt.title('Learning Curves for Neural Network Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/NN_Learning_Curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of model weights based on inverse RMSE for each forecasted month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the monthly performance metric files.\n",
    "directory = 'Segmented Approach Outputs'\n",
    "\n",
    "# The files are listed and sorted to ensure they are processed in chronological order.\n",
    "# Only files starting with 'performance_metrics_overall_' are considered.\n",
    "files = sorted([f for f in os.listdir(directory) if f.startswith('performance_metrics_overall_')])\n",
    "\n",
    "# Initialise a list to store the calculated weights for each month.\n",
    "monthly_weights = []\n",
    "\n",
    "# An iteration is performed over each file to calculate the model weights based on inverse RMSE.\n",
    "for file in files:\n",
    "    date_str = file.replace('performance_metrics_overall_', '').replace('.csv', '')\n",
    "    \n",
    "    # The performance metrics file is loaded into a DataFrame.\n",
    "    file_path = os.path.join(directory, file)\n",
    "    metrics_df = pd.read_csv(file_path)\n",
    "\n",
    "    # The RMSE values for each model (XGBoost, LSTM, and NN) are extracted.\n",
    "    rmse_xgboost = metrics_df['RMSE XGBoost'].values[0]\n",
    "    rmse_lstm = metrics_df['RMSE LSTM'].values[0]\n",
    "    rmse_nn = metrics_df['RMSE NN'].values[0]\n",
    "\n",
    "    # The inverse RMSE is calculated for each model.\n",
    "    inv_rmse_xgboost = 1 / rmse_xgboost\n",
    "    inv_rmse_lstm = 1 / rmse_lstm\n",
    "    inv_rmse_nn = 1 / rmse_nn\n",
    "\n",
    "    # The inverse RMSE values are summed to normalise the weights.\n",
    "    sum_inv_rmse = inv_rmse_xgboost + inv_rmse_lstm + inv_rmse_nn\n",
    "\n",
    "    # The weights for each model are calculated by normalising the inverse RMSE.\n",
    "    weight_xgboost = inv_rmse_xgboost / sum_inv_rmse\n",
    "    weight_lstm = inv_rmse_lstm / sum_inv_rmse\n",
    "    weight_nn = inv_rmse_nn / sum_inv_rmse\n",
    "\n",
    "    # The calculated weights are appended to the list, along with the corresponding date.\n",
    "    monthly_weights.append({\n",
    "        'Date': date_str,\n",
    "        'XGBoost Weight': weight_xgboost,\n",
    "        'LSTM Weight': weight_lstm,\n",
    "        'NN Weight': weight_nn\n",
    "    })\n",
    "\n",
    "# The list of weights is converted into a DataFrame for easy viewing and saving.\n",
    "weights_df = pd.DataFrame(monthly_weights)\n",
    "\n",
    "# The calculated weights for each month are displayed in the console.\n",
    "print(weights_df)\n",
    "\n",
    "# The DataFrame of weights is saved to a CSV file in the specified output directory.\n",
    "output_path = os.path.join('GHVT6_Outputs', 'model_weights_per_month.csv')\n",
    "weights_df.to_csv(output_path, index=False)\n",
    "print(f\"Weights per month have been saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of correlation between model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The predictions from the XGBoost, LSTM, and Neural Network (NN) models are averaged across all bootstrapped predictions.\n",
    "# This averaging is performed along the axis 0, which corresponds to the first axis (i.e., rows).\n",
    "xgb_predictions = np.mean(xgboost_bootstrap_preds, axis=0)\n",
    "lstm_predictions = np.mean(lstm_bootstrap_preds, axis=0)\n",
    "nn_predictions = np.mean(nn_bootstrap_preds, axis=0)\n",
    "\n",
    "# The averaged predictions from each model are combined into a single DataFrame.\n",
    "# Each column in the DataFrame corresponds to the predictions from one model: XGBoost, LSTM, and NN.\n",
    "predictions_df = pd.DataFrame({\n",
    "    'XGBoost': xgb_predictions,\n",
    "    'LSTM': lstm_predictions,\n",
    "    'NN': nn_predictions\n",
    "})\n",
    "\n",
    "# A correlation matrix is calculated to understand the relationships between the predictions of different models.\n",
    "# The correlation matrix quantifies how similar the predictions from different models are to each other.\n",
    "correlation_matrix = predictions_df.corr()\n",
    "\n",
    "# The calculated correlation matrix is printed to the console.\n",
    "# This provides an immediate view of the degree to which the models' predictions are correlated.\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Explained for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty DataFrame is initialised to store the results for all months.\n",
    "# This DataFrame will accumulate the results as predictions for each month are generated.\n",
    "all_results_df = pd.DataFrame()\n",
    "\n",
    "# A list of prediction dates is defined, corresponding to the specific time periods for which forecasts will be generated.\n",
    "# These dates will guide the iteration process.\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']\n",
    "\n",
    "# An iteration is performed over each prediction date to generate forecasts and aggregate results.\n",
    "for date in prediction_dates:\n",
    "    # [Your existing code for generating predictions]\n",
    "    \n",
    "    # The results for the current month are appended to the main DataFrame.\n",
    "    # This ensures that the results from all months are consolidated into a single DataFrame.\n",
    "    all_results_df = pd.concat([all_results_df, results_df])\n",
    "\n",
    "# The explained variance is calculated at an aggregated level for each month.\n",
    "# This is done for each prediction model (LSTM, NN, XGBoost, and Ensemble).\n",
    "explained_variance_lstm_df = calculate_aggregated_explained_variance_per_month(all_results_df, prediction_column='LSTM Prediction')\n",
    "explained_variance_nn_df = calculate_aggregated_explained_variance_per_month(all_results_df, prediction_column='NN Prediction')\n",
    "explained_variance_xgb_df = calculate_aggregated_explained_variance_per_month(all_results_df, prediction_column='XGBoost Prediction')\n",
    "explained_variance_ensemble_df = calculate_aggregated_explained_variance_per_month(all_results_df, prediction_column='Ensemble Prediction')\n",
    "\n",
    "# The results for each model are printed to the console.\n",
    "# This provides immediate feedback on the performance of each model across the different months.\n",
    "print(\"Aggregated Explained Variance per month - LSTM:\")\n",
    "print(explained_variance_lstm_df)\n",
    "print(\"Aggregated Explained Variance per month - NN:\")\n",
    "print(explained_variance_nn_df)\n",
    "print(\"Aggregated Explained Variance per month - XGBoost:\")\n",
    "print(explained_variance_xgb_df)\n",
    "print(\"Aggregated Explained Variance per month - Ensemble:\")\n",
    "print(explained_variance_ensemble_df)\n",
    "\n",
    "# The aggregated explained variance per month is saved to CSV files for each prediction model.\n",
    "# These files are stored in the specified output directory for further analysis and documentation.\n",
    "\n",
    "# LSTM model\n",
    "explained_variance_lstm_df.to_csv(f'{output_dir}/explained_variance_per_month_LSTM.csv', index=False)\n",
    "\n",
    "# NN model\n",
    "explained_variance_nn_df.to_csv(f'{output_dir}/explained_variance_per_month_NN.csv', index=False)\n",
    "\n",
    "# XGBoost model\n",
    "explained_variance_xgb_df.to_csv(f'{output_dir}/explained_variance_per_month_XGBoost.csv', index=False)\n",
    "\n",
    "# Ensemble model\n",
    "explained_variance_ensemble_df.to_csv(f'{output_dir}/explained_variance_per_month_Ensemble.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics across demographic groups per country of citizenship, and across countries for overall asylum applications submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# The output directory is defined where all the result files will be saved.\n",
    "# If the directory does not already exist, it will be created.\n",
    "output_dir = \"Segmented Approach Outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# A list of prediction dates is defined, corresponding to the time periods used in the main analysis.\n",
    "# These dates will guide the iteration process when loading and processing the results files.\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']  \n",
    "\n",
    "# A function is defined to calculate key performance metrics: MSE, RMSE, MAE, and MDAE.\n",
    "# These metrics provide insights into the accuracy and error distribution of the prediction models.\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \n",
    "    ### Calculate the Mean Squared Error (MSE), Root Mean Squared Error (RMSE), ###\n",
    "    ### Mean Absolute Error (MAE), and Median Absolute Error (MDAE) ### \n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)  # Mean Squared Error\n",
    "    rmse = mse ** 0.5  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_true, y_pred)  # Mean Absolute Error\n",
    "    mdae = median_absolute_error(y_true, y_pred)  # Median Absolute Error\n",
    "    return mse, rmse, mae, mdae  # Return all calculated metrics\n",
    "\n",
    "# An iteration is performed over each prediction date to load and process the corresponding results files.\n",
    "# For each date, the relevant data is loaded, metrics are calculated, and the results are saved.\n",
    "for date in prediction_dates:\n",
    "    # The results file for the current prediction date is loaded into a DataFrame.\n",
    "    results_df = pd.read_csv(f'{output_dir}/forecasts_with_uncertainty_group_level_{date}.csv')\n",
    "\n",
    "    # Lists are initialised to collect all true values and predictions across all countries.\n",
    "    all_true_values = []\n",
    "    all_ensemble_preds = []\n",
    "    all_xgb_preds = []\n",
    "    all_lstm_preds = []\n",
    "    all_nn_preds = []\n",
    "\n",
    "    # Performance metrics are calculated across all groups per country.\n",
    "    # The data is grouped by 'country' to calculate metrics for each country separately.\n",
    "    grouped_results = results_df.groupby('country')\n",
    "\n",
    "    # A list is created to store the calculated metrics for each country.\n",
    "    metrics_per_country = []\n",
    "\n",
    "    # Metrics are calculated for each country across its groups.\n",
    "    for country, country_data in grouped_results:\n",
    "        y_true = country_data['True Values'].values  # Actual observed values\n",
    "        y_pred_ensemble = country_data['Ensemble Prediction'].values  # Ensemble model predictions\n",
    "        y_pred_xgb = country_data['XGBoost Prediction'].values  # XGBoost model predictions\n",
    "        y_pred_lstm = country_data['LSTM Prediction'].values  # LSTM model predictions\n",
    "        y_pred_nn = country_data['NN Prediction'].values  # Neural Network model predictions\n",
    "\n",
    "        # True values and predictions for each country are collected.\n",
    "        all_true_values.extend(y_true)\n",
    "        all_ensemble_preds.extend(y_pred_ensemble)\n",
    "        all_xgb_preds.extend(y_pred_xgb)\n",
    "        all_lstm_preds.extend(y_pred_lstm)\n",
    "        all_nn_preds.extend(y_pred_nn)\n",
    "\n",
    "        # Metrics are calculated for each country across its groups using the defined function.\n",
    "        mse_ensemble, rmse_ensemble, mae_ensemble, mdae_ensemble = calculate_metrics(y_true, y_pred_ensemble)\n",
    "        mse_xgb, rmse_xgb, mae_xgb, mdae_xgb = calculate_metrics(y_true, y_pred_xgb)\n",
    "        mse_lstm, rmse_lstm, mae_lstm, mdae_lstm = calculate_metrics(y_true, y_pred_lstm)\n",
    "        mse_nn, rmse_nn, mae_nn, mdae_nn = calculate_metrics(y_true, y_pred_nn)\n",
    "\n",
    "        # The calculated metrics are appended to the list for each country.\n",
    "        metrics_per_country.append({\n",
    "            'Country': country,\n",
    "            'MSE Ensemble': mse_ensemble,\n",
    "            'RMSE Ensemble': rmse_ensemble,\n",
    "            'MAE Ensemble': mae_ensemble,\n",
    "            'MDAE Ensemble': mdae_ensemble,\n",
    "            'MSE XGBoost': mse_xgb,\n",
    "            'RMSE XGBoost': rmse_xgb,\n",
    "            'MAE XGBoost': mae_xgb,\n",
    "            'MDAE XGBoost': mdae_xgb,\n",
    "            'MSE LSTM': mse_lstm,\n",
    "            'RMSE LSTM': rmse_lstm,\n",
    "            'MAE LSTM': mae_lstm,\n",
    "            'MDAE LSTM': mdae_lstm,\n",
    "            'MSE NN': mse_nn,\n",
    "            'RMSE NN': rmse_nn,\n",
    "            'MAE NN': mae_nn,\n",
    "            'MDAE NN': mdae_nn\n",
    "        })\n",
    "\n",
    "    # The list of country metrics is converted to a DataFrame and the values are rounded to 2 decimal places.\n",
    "    metrics_country_df = pd.DataFrame(metrics_per_country).round(2)\n",
    "\n",
    "    # The metrics for each country are saved to a CSV file for the current month.\n",
    "    metrics_country_df.to_csv(f'{output_dir}/performance_metrics_across_countries_{date}.csv', index=False)\n",
    "\n",
    "    # Overall performance metrics across all countries are calculated by aggregating the true values and predictions.\n",
    "    overall_mse_ensemble, overall_rmse_ensemble, overall_mae_ensemble, overall_mdae_ensemble = calculate_metrics(all_true_values, all_ensemble_preds)\n",
    "    overall_mse_xgb, overall_rmse_xgb, overall_mae_xgb, overall_mdae_xgb = calculate_metrics(all_true_values, all_xgb_preds)\n",
    "    overall_mse_lstm, overall_rmse_lstm, overall_mae_lstm, overall_mdae_lstm = calculate_metrics(all_true_values, all_lstm_preds)\n",
    "    overall_mse_nn, overall_rmse_nn, overall_mae_nn, overall_mdae_nn = calculate_metrics(all_true_values, all_nn_preds)\n",
    "\n",
    "    # The overall metrics across all countries are stored in a DataFrame for the current month and rounded to 2 decimal places.\n",
    "    overall_metrics_df = pd.DataFrame({\n",
    "        'Model': ['Ensemble', 'XGBoost', 'LSTM', 'NN'],\n",
    "        'MSE': [overall_mse_ensemble, overall_mse_xgb, overall_mse_lstm, overall_mse_nn],\n",
    "        'RMSE': [overall_rmse_ensemble, overall_rmse_xgb, overall_rmse_lstm, overall_rmse_nn],\n",
    "        'MAE': [overall_mae_ensemble, overall_mae_xgb, overall_mae_lstm, overall_mae_nn],\n",
    "        'MDAE': [overall_mdae_ensemble, overall_mdae_xgb, overall_mdae_lstm, overall_mdae_nn]\n",
    "    }).round(2)\n",
    "\n",
    "    # The overall performance metrics across all countries are saved to a CSV file for the current month.\n",
    "    overall_metrics_df.to_csv(f'{output_dir}/overall_performance_metrics_{date}.csv', index=False)\n",
    "\n",
    "    # The results for the current month are displayed in the console.\n",
    "    print(f\"Performance Metrics Across Groups Per Country for {date}:\")\n",
    "    print(metrics_country_df)\n",
    "\n",
    "    print(f\"\\nOverall Performance Metrics Across All Countries for {date}:\")\n",
    "    print(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance metrics per demographic group across countries of citizenship, and across countries for the grand total of asylum applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# The output directory is defined where all the result files will be saved.\n",
    "# If the directory does not exist, it is created automatically.\n",
    "output_dir = \"Segmented Approach Outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# A list of prediction dates is defined, corresponding to the dates used in the main code.\n",
    "# These dates represent the different time periods for which performance metrics will be calculated.\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']  \n",
    "\n",
    "# A function is defined to calculate key performance metrics: MSE, RMSE, MAE, and MDAE.\n",
    "# The function takes the true values and predicted values as input and returns the calculated metrics.\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \n",
    "    #### Calculate the Mean Squared Error (MSE), Root Mean Squared Error (RMSE),\n",
    "    #### Mean Absolute Error (MAE), and Median Absolute Error (MDAE).\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)  # Mean Squared Error\n",
    "    rmse = mse ** 0.5  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_true, y_pred)  # Mean Absolute Error\n",
    "    mdae = median_absolute_error(y_true, y_pred)  # Median Absolute Error\n",
    "    return mse, rmse, mae, mdae  # Return all calculated metrics\n",
    "\n",
    "# An iteration is performed over each prediction date to load and process the corresponding result files.\n",
    "# For each date, the relevant data is loaded, metrics are calculated, and the results are saved.\n",
    "for date in prediction_dates:\n",
    "    # The results file for the current prediction date is loaded into a DataFrame.\n",
    "    results_df = pd.read_csv(f'{output_dir}/forecasts_with_uncertainty_group_level_{date}.csv')\n",
    "\n",
    "    # Lists are initialised to collect all true values and predictions across all countries.\n",
    "    all_true_values = []\n",
    "    all_ensemble_preds = []\n",
    "    all_xgb_preds = []\n",
    "    all_lstm_preds = []\n",
    "    all_nn_preds = []\n",
    "\n",
    "    # Performance metrics are calculated across all groups per country.\n",
    "    # The data is grouped by 'country' to calculate metrics for each country separately.\n",
    "    grouped_results = results_df.groupby('country')\n",
    "\n",
    "    # A list is created to store the calculated metrics for each country.\n",
    "    metrics_per_country = []\n",
    "\n",
    "    # The calculation of metrics for each country across its groups is performed.\n",
    "    for country, country_data in grouped_results:\n",
    "        y_true = country_data['True Values'].values  # True values for the country\n",
    "        y_pred_ensemble = country_data['Ensemble Prediction'].values  # Ensemble model predictions\n",
    "        y_pred_xgb = country_data['XGBoost Prediction'].values  # XGBoost model predictions\n",
    "        y_pred_lstm = country_data['LSTM Prediction'].values  # LSTM model predictions\n",
    "        y_pred_nn = country_data['NN Prediction'].values  # Neural Network model predictions\n",
    "\n",
    "        # True values and predictions for each country are collected.\n",
    "        all_true_values.extend(y_true)\n",
    "        all_ensemble_preds.extend(y_pred_ensemble)\n",
    "        all_xgb_preds.extend(y_pred_xgb)\n",
    "        all_lstm_preds.extend(y_pred_lstm)\n",
    "        all_nn_preds.extend(y_pred_nn)\n",
    "\n",
    "        # Metrics are calculated for each country across its groups using the defined function.\n",
    "        mse_ensemble, rmse_ensemble, mae_ensemble, mdae_ensemble = calculate_metrics(y_true, y_pred_ensemble)\n",
    "        mse_xgb, rmse_xgb, mae_xgb, mdae_xgb = calculate_metrics(y_true, y_pred_xgb)\n",
    "        mse_lstm, rmse_lstm, mae_lstm, mdae_lstm = calculate_metrics(y_true, y_pred_lstm)\n",
    "        mse_nn, rmse_nn, mae_nn, mdae_nn = calculate_metrics(y_true, y_pred_nn)\n",
    "\n",
    "        # The calculated metrics are appended to the list for each country.\n",
    "        metrics_per_country.append({\n",
    "            'Country': country,\n",
    "            'MSE Ensemble': mse_ensemble,\n",
    "            'RMSE Ensemble': rmse_ensemble,\n",
    "            'MAE Ensemble': mae_ensemble,\n",
    "            'MDAE Ensemble': mdae_ensemble,\n",
    "            'MSE XGBoost': mse_xgb,\n",
    "            'RMSE XGBoost': rmse_xgb,\n",
    "            'MAE XGBoost': mae_xgb,\n",
    "            'MDAE XGBoost': mdae_xgb,\n",
    "            'MSE LSTM': mse_lstm,\n",
    "            'RMSE LSTM': rmse_lstm,\n",
    "            'MAE LSTM': mae_lstm,\n",
    "            'MDAE LSTM': mdae_lstm,\n",
    "            'MSE NN': mse_nn,\n",
    "            'RMSE NN': rmse_nn,\n",
    "            'MAE NN': mae_nn,\n",
    "            'MDAE NN': mdae_nn\n",
    "        })\n",
    "\n",
    "    # The list of country metrics is converted to a DataFrame and the values are rounded to 2 decimal places.\n",
    "    metrics_country_df = pd.DataFrame(metrics_per_country).round(2)\n",
    "\n",
    "    # The metrics for each country are saved to a CSV file for the current month.\n",
    "    metrics_country_df.to_csv(f'{output_dir}/performance_metrics_across_countries_{date}.csv', index=False)\n",
    "\n",
    "    # Overall performance metrics across all countries are calculated by aggregating the true values and predictions.\n",
    "    overall_mse_ensemble, overall_rmse_ensemble, overall_mae_ensemble, overall_mdae_ensemble = calculate_metrics(all_true_values, all_ensemble_preds)\n",
    "    overall_mse_xgb, overall_rmse_xgb, overall_mae_xgb, overall_mdae_xgb = calculate_metrics(all_true_values, all_xgb_preds)\n",
    "    overall_mse_lstm, overall_rmse_lstm, overall_mae_lstm, overall_mdae_lstm = calculate_metrics(all_true_values, all_lstm_preds)\n",
    "    overall_mse_nn, overall_rmse_nn, overall_mae_nn, overall_mdae_nn = calculate_metrics(all_true_values, all_nn_preds)\n",
    "\n",
    "    # The overall metrics across all countries are stored in a DataFrame for the current month and rounded to 2 decimal places.\n",
    "    overall_metrics_df = pd.DataFrame({\n",
    "        'Model': ['Ensemble', 'XGBoost', 'LSTM', 'NN'],\n",
    "        'MSE': [overall_mse_ensemble, overall_mse_xgb, overall_mse_lstm, overall_mse_nn],\n",
    "        'RMSE': [overall_rmse_ensemble, overall_rmse_xgb, overall_rmse_lstm, overall_rmse_nn],\n",
    "        'MAE': [overall_mae_ensemble, overall_mae_xgb, overall_mae_lstm, overall_mae_nn],\n",
    "        'MDAE': [overall_mdae_ensemble, overall_mdae_xgb, overall_mdae_lstm, overall_mdae_nn]\n",
    "    }).round(2)\n",
    "\n",
    "    # The overall performance metrics across all countries are saved to a CSV file for the current month.\n",
    "    overall_metrics_df.to_csv(f'{output_dir}/overall_performance_metrics_{date}.csv', index=False)\n",
    "\n",
    "    # The results for the current month are displayed in the console.\n",
    "    print(f\"Performance Metrics Across Groups Per Country for {date}:\")\n",
    "    print(metrics_country_df)\n",
    "\n",
    "    print(f\"\\nOverall Performance Metrics Across All Countries for {date}:\")\n",
    "    print(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble prediction errors across countries of citizenship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# The output directory is defined where all the result files will be saved.\n",
    "output_dir = \"Segmented Approach Outputs\"\n",
    "\n",
    "# A list of prediction dates is specified, corresponding to the dates used in the main analysis.\n",
    "# These dates indicate the time periods for which error calculations will be performed.\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01'] \n",
    "\n",
    "# An empty DataFrame is initialised to hold the overall errors across all countries and dates.\n",
    "# This DataFrame will be incrementally built as each prediction date is processed.\n",
    "overall_errors = pd.DataFrame()\n",
    "\n",
    "# An iteration is performed over each prediction date to process the corresponding results file.\n",
    "for date in prediction_dates:\n",
    "    # The file path for the current prediction date's aggregation results is constructed.\n",
    "    # It is checked whether the file exists before attempting to load it.\n",
    "    filepath = f'{output_dir}/country_level_aggregation_{date}.csv'\n",
    "    if os.path.exists(filepath):\n",
    "        data_df = pd.read_csv(filepath)  # The data is loaded into a DataFrame.\n",
    "\n",
    "        # The error is calculated as the difference between the true values and the ensemble predictions.\n",
    "        # A new column 'Ensemble Error' is added to the DataFrame to store these error values.\n",
    "        data_df['Ensemble Error'] = data_df['True Values'] - data_df['Ensemble Prediction']\n",
    "\n",
    "        # The updated DataFrame, now containing the error calculations, is saved back to a CSV file.\n",
    "        data_df.to_csv(f'{output_dir}/country_level_aggregation_with_error_{date}.csv', index=False)\n",
    "        \n",
    "        # The updated DataFrame is appended to the overall errors DataFrame.\n",
    "        # This step aggregates the error data across all prediction dates.\n",
    "        overall_errors = pd.concat([overall_errors, data_df], ignore_index=True)\n",
    "\n",
    "# The errors are summarised by country and date.\n",
    "# The sum of errors is calculated for each country and date combination, and the results are reset to a new index.\n",
    "country_errors = overall_errors.groupby(['country', 'year_month'])['Ensemble Error'].sum().reset_index()\n",
    "\n",
    "# The summarised errors DataFrame is sorted first by date ('year_month') and then by country.\n",
    "# This ensures that the data is organised chronologically and by country for easier analysis.\n",
    "country_errors = country_errors.sort_values(by=['year_month', 'country'])\n",
    "\n",
    "# The sorted errors by country and date are saved to a CSV file.\n",
    "# This file contains the detailed error information for further examination and documentation.\n",
    "country_errors.to_csv(f'{output_dir}/errors_by_country_and_date.csv', index=False)\n",
    "\n",
    "# The overall error across all countries and dates is calculated by summing the 'Ensemble Error' column.\n",
    "# This provides a single metric indicating the total error in the predictions over the entire period.\n",
    "total_error = overall_errors['Ensemble Error'].sum()\n",
    "print(f\"Total Error across all countries and dates: {total_error}\")\n",
    "\n",
    "# The total overall error is saved to a separate CSV file for documentation purposes.\n",
    "overall_error_df = pd.DataFrame({'Total Error': [total_error]})\n",
    "overall_error_df.to_csv(f'{output_dir}/total_error_across_all_countries.csv', index=False)\n",
    "\n",
    "# The summarised errors by country and date are displayed.\n",
    "# This provides an immediate view of the error distribution across the different countries and time periods.\n",
    "print(\"Errors by country and date:\")\n",
    "print(country_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising asylum application forecasts with uncertainty intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# The global font for all plots is set to Times New Roman, with a default font size of 12.\n",
    "# This ensures consistency in the appearance of the plots.\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12  \n",
    "\n",
    "# The dataset containing the actual data is loaded from a CSV file.\n",
    "# This dataset serves as the base for merging with forecast data.\n",
    "data_agg = pd.read_csv('final_thesis_data.csv')\n",
    "\n",
    "# An output directory is created for saving the generated graphs.\n",
    "# This directory will store the final visualisations of the forecasting results.\n",
    "output_dir = \"Graphs - Country-Sex-AgeGroup Forecasting Output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# The 'year_month' column is converted to datetime format to ensure proper handling of time series data.\n",
    "data_agg['year_month'] = pd.to_datetime(data_agg['year_month'])\n",
    "\n",
    "# A dictionary is defined to map forecast dates to their corresponding CSV files.\n",
    "# These files contain the forecasted data, including predictions and uncertainty intervals.\n",
    "forecast_files = {\n",
    "    '2024-01-01': 'Segmented Approach Outputs/forecasts_with_uncertainty_group_level_2024-01-01.csv',\n",
    "    '2024-02-01': 'Segmented Approach Outputs/forecasts_with_uncertainty_group_level_2024-02-01.csv',\n",
    "    '2024-03-01': 'Segmented Approach Outputs/forecasts_with_uncertainty_group_level_2024-03-01.csv'\n",
    "}\n",
    "\n",
    "# A new DataFrame is created to hold the combined actual and forecasted data.\n",
    "# This DataFrame initially includes the actual data only.\n",
    "all_results_df = data_agg[['country', 'sex', 'age_group', 'year_month', 'asy_applications']].copy()\n",
    "\n",
    "# The forecast data from each file is merged with the actual data.\n",
    "# This loop iterates through the forecast files, merges them with the actual data, and adds uncertainty intervals.\n",
    "for date, forecast_file in forecast_files.items():\n",
    "    if os.path.exists(forecast_file):\n",
    "        forecast_df = pd.read_csv(forecast_file)\n",
    "        forecast_df['year_month'] = pd.to_datetime(date)  # The forecast date is set.\n",
    "        # Column names are updated to include the forecast date, differentiating predictions and confidence intervals.\n",
    "        forecast_df.rename(columns={\n",
    "            'Ensemble Prediction': f'Ensemble Prediction_{date}',\n",
    "            'Ensemble Lower CI': f'Lower CI_{date}',\n",
    "            'Ensemble Upper CI': f'Upper CI_{date}'\n",
    "        }, inplace=True)\n",
    "        # The forecast data is merged with the actual data based on common identifiers: country, sex, age_group, and year_month.\n",
    "        all_results_df = pd.merge(\n",
    "            all_results_df,\n",
    "            forecast_df[['country', 'sex', 'age_group', 'year_month', f'Ensemble Prediction_{date}', f'Lower CI_{date}', f'Upper CI_{date}']],\n",
    "            on=['country', 'sex', 'age_group', 'year_month'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        # A warning is printed if a forecast file is missing.\n",
    "        print(f\"Forecast file for {date} not found.\")\n",
    "\n",
    "# The merged data, which now includes both actual and forecasted values with uncertainty intervals, is saved to a CSV file.\n",
    "merged_data_filepath = os.path.join(output_dir, 'uncertainty_segmented_approach.csv')\n",
    "all_results_df.to_csv(merged_data_filepath, index=False)\n",
    "print(f\"Merged data saved to {merged_data_filepath}\")\n",
    "\n",
    "# A new directory is created specifically for storing the individual plots.\n",
    "individual_plots_dir = \"Forecasts' Plots with Uncertainty - Segmented Forecasting Approach\" \n",
    "os.makedirs(individual_plots_dir, exist_ok=True)\n",
    "\n",
    "# Plots are generated for each country, showing the true values, predictions, and uncertainty intervals across different groups.\n",
    "for country, country_data in all_results_df.groupby('country'):\n",
    "    # A figure with subplots is created for each country, organised by sex and age group.\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 12), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()  # The axes array is flattened for easy iteration.\n",
    "    for i, ((sex, age_group), group_data) in enumerate(country_data.groupby(['sex', 'age_group'])):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # The true asylum application values are plotted across the entire period.\n",
    "        ax.plot(group_data['year_month'], group_data['asy_applications'], label='True Values', linestyle='-', color='blue')\n",
    "        \n",
    "        # Forecast values and confidence intervals are plotted for the forecast period only.\n",
    "        forecast_mask = group_data['year_month'].isin(pd.to_datetime(list(forecast_files.keys())))\n",
    "        if forecast_mask.any():\n",
    "            ax.plot(group_data['year_month'], group_data[['Ensemble Prediction_2024-01-01', 'Ensemble Prediction_2024-02-01', 'Ensemble Prediction_2024-03-01']].ffill(axis=1).bfill(axis=1).iloc[:, -1], label='Predictions', linestyle='--', color='red', marker=None)\n",
    "            ax.fill_between(group_data['year_month'], group_data[['Lower CI_2024-01-01', 'Lower CI_2024-02-01', 'Lower CI_2024-03-01']].min(axis=1), group_data[['Upper CI_2024-01-01', 'Upper CI_2024-02-01', 'Upper CI_2024-03-01']].max(axis=1), alpha=0.2, color='red', label='95% CI')\n",
    "        \n",
    "        ax.set_title(f'{sex}, {age_group}')\n",
    "        ax.set_xlabel('Year Month')\n",
    "        ax.set_ylabel('Asylum Applications')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # An inset plot is added to zoom in on the forecast period.\n",
    "        ax_inset = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper center', borderpad=2)\n",
    "        ax_inset.plot(group_data['year_month'], group_data['asy_applications'], linestyle='-', color='blue')\n",
    "        if forecast_mask.any():\n",
    "            ax_inset.plot(group_data['year_month'], group_data[['Ensemble Prediction_2024-01-01', 'Ensemble Prediction_2024-02-01', 'Ensemble Prediction_2024-03-01']].ffill(axis=1).bfill(axis=1).iloc[:, -1], linestyle='--', color='red', marker=None)\n",
    "            ax_inset.fill_between(group_data['year_month'], group_data[['Lower CI_2024-01-01', 'Lower CI_2024-02-01', 'Lower CI_2024-03-01']].min(axis=1), group_data[['Upper CI_2024-01-01', 'Upper CI_2024-02-01', 'Upper CI_2024-03-01']].max(axis=1), alpha=0.5, color='red')\n",
    "        \n",
    "        # The inset plot is configured to focus on the forecast period.\n",
    "        ax_inset.set_xlim(pd.to_datetime('2023-12-01'), pd.to_datetime('2024-03-31'))\n",
    "        ax_inset.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        ax_inset.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        fig.autofmt_xdate()\n",
    "\n",
    "    # The layout of the figure is adjusted to ensure plots are properly arranged and not overlapping.\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # The plot for the current country is saved to the designated directory.\n",
    "    plt.savefig(f'{individual_plots_dir}/Forecast_Comparison_{country}.png')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Plots saved in directory: {individual_plots_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
