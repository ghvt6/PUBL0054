{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing and Managing a Cached Function for Optimised Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_function(args):\n",
    "    # This function is decorated with @lru_cache, which enables caching of its results.\n",
    "    # The 'maxsize' parameter specifies that up to 128 results can be stored in the cache.\n",
    "    # Caching can significantly improve performance by avoiding redundant calculations for repeated inputs.\n",
    "    # When the function is called with the same arguments, the cached result is returned instead of recomputing it.\n",
    "    # The function implementation should be inserted here.\n",
    "    pass\n",
    "\n",
    "# The cache associated with the 'cached_function' is cleared.\n",
    "# This is useful in scenarios where the cached results are no longer valid or when it is necessary to free up memory.\n",
    "cached_function.cache_clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of agggregated approach for forecasting asylum applications using ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, median_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Attention, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Setting the font to Times New Roman globally\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Ensure reproducibility with TensorFlow 2.x\n",
    "tf.keras.utils.set_random_seed(random_seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Create output directory for graphs\n",
    "output_dir = \"Aggregated Approach Outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"final_thesis_data.csv\")\n",
    "data['year_month'] = pd.to_datetime(data['year_month'])\n",
    "data = data.sort_values(by=['country', 'year_month'])\n",
    "\n",
    "# Aggregate data by country and year_month\n",
    "data_agg = data.groupby(['country', 'year_month']).agg({\n",
    "    'asy_applications': 'sum',\n",
    "    'illegal_border_crossings': 'mean',\n",
    "    'push_factor_index': 'mean',\n",
    "    'push_factor_index_high_level': 'mean',\n",
    "    'deaths_civilians': 'mean',\n",
    "    'gdp_per_capita_current_usd': 'mean',\n",
    "    'gdp_per_capita_growth': 'mean',\n",
    "    'regime_end_type': 'mean',\n",
    "    'state_fiscal_source_revenue': 'mean',\n",
    "    'state_authority_over_territory': 'mean',\n",
    "    'political_polarisation': 'mean',\n",
    "    'political_violence': 'mean',\n",
    "    'domestic_autonomy': 'mean',\n",
    "    'rule_of_law': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Define variables to lag and create lagged variables\n",
    "variables_to_lag = {\n",
    "    \"illegal_border_crossings\": [3],\n",
    "    \"push_factor_index\": [3],\n",
    "    \"push_factor_index_high_level\": [2],\n",
    "    \"deaths_civilians\": [12],\n",
    "    \"gdp_per_capita_current_usd\": [12],\n",
    "    \"gdp_per_capita_growth\": [12],\n",
    "    \"regime_end_type\": [0],\n",
    "    \"state_fiscal_source_revenue\": [0],\n",
    "    \"state_authority_over_territory\": [0],\n",
    "    \"political_polarisation\": [0],\n",
    "    \"political_violence\": [0],\n",
    "    \"domestic_autonomy\": [0],\n",
    "    \"rule_of_law\": [0],\n",
    "}\n",
    "\n",
    "# Adjust lagged variables at the country level\n",
    "for var, lags in variables_to_lag.items():\n",
    "    for lag in lags:\n",
    "        lagged_var_name = f\"{var}_lag_{lag}\"\n",
    "        data_agg[lagged_var_name] = data_agg.groupby(['country'])[var].shift(lag)\n",
    "\n",
    "# Define rolling window sizes\n",
    "rolling_windows = [3, 6, 12, 24]\n",
    "\n",
    "# Calculate rolling mean and standard deviation for asylum applications\n",
    "for window in rolling_windows:\n",
    "    data_agg[f'asy_applications_rolling_mean_{window}'] = data_agg.groupby(['country'])['asy_applications'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    data_agg[f'asy_applications_rolling_sd_{window}'] = data_agg.groupby(['country'])['asy_applications'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "\n",
    "# Calculate rolling mean and standard deviation for push_factor_index\n",
    "for window in rolling_windows:\n",
    "    data_agg[f'push_factor_index_rolling_mean_{window}'] = data_agg.groupby(['country'])['push_factor_index'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    data_agg[f'push_factor_index_rolling_sd_{window}'] = data_agg.groupby(['country'])['push_factor_index'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "\n",
    "# Calculate exponential moving average for asylum applications\n",
    "data_agg['asy_applications_ewm_mean'] = data_agg.groupby(['country'])['asy_applications'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "data_agg['asy_applications_ewm_std'] = data_agg.groupby(['country'])['asy_applications'].transform(lambda x: x.ewm(span=12, adjust=False).std())\n",
    "\n",
    "# Add temporal features\n",
    "data_agg['month'] = data_agg['year_month'].dt.month\n",
    "data_agg['month_sin'] = np.sin(2 * np.pi * data_agg['month'] / 12)\n",
    "data_agg['month_cos'] = np.cos(2 * np.pi * data_agg['month'] / 12)\n",
    "\n",
    "# Define the predictors and the target variable\n",
    "predictors = [\n",
    "    \"illegal_border_crossings_lag_3\", \"push_factor_index_lag_3\",\n",
    "    \"push_factor_index_high_level_lag_2\", \"deaths_civilians_lag_12\",\n",
    "    \"gdp_per_capita_current_usd_lag_12\", \"gdp_per_capita_growth_lag_12\",\n",
    "    \"regime_end_type_lag_0\", \"state_fiscal_source_revenue_lag_0\",\n",
    "    \"state_authority_over_territory_lag_0\", \"political_polarisation_lag_0\",\n",
    "    \"political_violence_lag_0\", \"domestic_autonomy_lag_0\",\n",
    "    \"rule_of_law_lag_0\",\n",
    "    \"asy_applications_rolling_mean_3\", \"asy_applications_rolling_sd_3\",\n",
    "    \"asy_applications_rolling_mean_6\", \"asy_applications_rolling_sd_6\",\n",
    "    \"asy_applications_rolling_mean_12\", \"asy_applications_rolling_sd_12\",\n",
    "    \"asy_applications_rolling_mean_24\", \"asy_applications_rolling_sd_24\",\n",
    "    \"push_factor_index_rolling_mean_3\", \"push_factor_index_rolling_sd_3\",\n",
    "    \"push_factor_index_rolling_mean_6\", \"push_factor_index_rolling_sd_6\",\n",
    "    \"push_factor_index_rolling_mean_12\", \"push_factor_index_rolling_sd_12\",\n",
    "    \"push_factor_index_rolling_mean_24\", \"push_factor_index_rolling_sd_24\",\n",
    "    \"asy_applications_ewm_mean\", \"asy_applications_ewm_std\",\n",
    "    \"month_sin\", \"month_cos\"\n",
    "]\n",
    "target_var = \"asy_applications\"\n",
    "\n",
    "# Handle missing values using Iterative Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=random_seed)\n",
    "data_agg[predictors] = imputer.fit_transform(data_agg[predictors])\n",
    "\n",
    "# Scale the predictors and target variable\n",
    "scaler_x = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "data_agg[predictors] = scaler_x.fit_transform(data_agg[predictors])\n",
    "data_agg[target_var] = scaler_y.fit_transform(data_agg[target_var].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Function to create and train the LSTM model with L2 regularization, increased dropout rate, and attention mechanism\n",
    "def create_and_train_lstm_with_attention(X_train, y_train, X_val, y_val, units_1=32, units_2=32, dropout_rate=0.5, learning_rate=0.001, l2_lambda=0.01):\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    lstm_out = LSTM(units_1, return_sequences=True, kernel_regularizer=l2(l2_lambda))(inputs)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    lstm_out = LSTM(units_2, return_sequences=True, kernel_regularizer=l2(l2_lambda))(lstm_out)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    attention_out = Attention()([lstm_out, lstm_out])\n",
    "    attention_out = Dense(units_2, activation='relu')(attention_out)\n",
    "    attention_out = Flatten()(attention_out)\n",
    "    \n",
    "    outputs = Dense(1, kernel_regularizer=l2(l2_lambda), activation='linear')(attention_out)  # Use linear activation\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping_lstm], verbose=0)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to create and train a neural network model with best hyperparameters, L2 regularization, and increased dropout rate\n",
    "def create_best_nn_model(X_train, y_train, X_val, y_val, dropout_rate=0.3, units_1=128, units_2=64, learning_rate=0.001, l2_lambda=0.01):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),  # Ensure this is the first layer\n",
    "        Dense(units_1, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units_2, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, kernel_regularizer=l2(l2_lambda), activation='linear')  # Use linear activation\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    early_stopping_nn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping_nn], verbose=0)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to train an XGBoost model with the best hyperparameters\n",
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    model = xgb.XGBRegressor(\n",
    "        learning_rate=0.2,\n",
    "        max_depth=7,\n",
    "        n_estimators=100,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=random_seed,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    return model \n",
    "\n",
    "# Function to split data for training and validation\n",
    "def split_train_val(data, quarter='Q1'):\n",
    "    data['quarter'] = data['year_month'].dt.to_period('Q')\n",
    "    val_data = data[data['quarter'].astype(str).str.endswith(quarter)]\n",
    "    train_data = data[~data.index.isin(val_data.index)]\n",
    "    return train_data, val_data\n",
    "\n",
    "# Function to calculate confidence intervals\n",
    "def calculate_confidence_intervals(predictions, confidence_level=0.95):\n",
    "    ### Calculate the confidence intervals for the predictions.###\n",
    "    mean_preds = np.mean(predictions, axis=0)\n",
    "    std_dev = np.std(predictions, axis=0)\n",
    "    z_score = 1.96  # Corresponds to 95% confidence interval\n",
    "    lower_bound = mean_preds - z_score * std_dev\n",
    "    upper_bound = mean_preds + z_score * std_dev\n",
    "    return mean_preds, lower_bound, upper_bound\n",
    "\n",
    "# Function for bootstrapping predictions\n",
    "def bootstrap_predictions(model_func, X_train, y_train, X_val, y_val, X_test, n_iterations=20):\n",
    "    #### Generate bootstrapped predictions for a given model function.###\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Resample with replacement\n",
    "        train_indices = np.random.choice(range(len(X_train)), size=len(X_train), replace=True)\n",
    "        X_train_resample = X_train[train_indices]\n",
    "        y_train_resample = y_train[train_indices]\n",
    "\n",
    "        # Reshape for LSTM if needed\n",
    "        if model_func.__name__ == 'create_and_train_lstm_with_attention':\n",
    "            X_train_resample = np.reshape(X_train_resample, (X_train_resample.shape[0], 1, X_train_resample.shape[1]))\n",
    "            X_val_resample = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "            X_test_resample = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "        else:\n",
    "            X_val_resample = X_val  # No reshaping needed for non-LSTM models\n",
    "            X_test_resample = X_test\n",
    "\n",
    "        # Train model on resampled data\n",
    "        if model_func.__name__ in ['create_and_train_lstm_with_attention', 'create_best_nn_model']:\n",
    "            model, _ = model_func(X_train_resample, y_train_resample, X_val_resample, y_val)\n",
    "        else:\n",
    "            model = model_func(X_train_resample, y_train_resample, X_val_resample, y_val)\n",
    "\n",
    "        # Predict on test data\n",
    "        preds = model.predict(X_test_resample).flatten()\n",
    "        predictions.append(preds)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Function to compute weights based on RMSE\n",
    "def calculate_weights(rmse_values):\n",
    "    ## Calculate weights inversely proportional to the RMSE.###\n",
    "    inv_rmse = 1 / np.array(rmse_values)\n",
    "    return inv_rmse / np.sum(inv_rmse)\n",
    "\n",
    "# Function to plot combined residuals over time\n",
    "def plot_combined_residuals(all_residuals):\n",
    "    countries = all_residuals['country'].unique()\n",
    "    for country in countries:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        group_data = all_residuals[all_residuals['country'] == country]\n",
    "\n",
    "        # Plot residuals for each dataset with different markers and colors\n",
    "        train_data = group_data[group_data['data_split'] == 'train']\n",
    "        val_data = group_data[group_data['data_split'] == 'val']\n",
    "        test_data = group_data[group_data['data_split'] == 'test']\n",
    "\n",
    "        plt.scatter(train_data['year_month'], train_data['Residuals'], alpha=0.5, label='Train', marker='o', color='blue')\n",
    "        plt.scatter(val_data['year_month'], val_data['Residuals'], alpha=0.5, label='Validation', marker='x', color='green')\n",
    "        plt.scatter(test_data['year_month'], test_data['Residuals'], alpha=0.5, label='Test', marker='^', color='red')\n",
    "\n",
    "        plt.title(f'Residuals - {country}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/Residuals_{country}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Function to plot average feature importances for each month\n",
    "def plot_aggregated_feature_importances(xgboost_models, predictors, output_dir, month):\n",
    "    # Initialise a list to hold the aggregated importances\n",
    "    aggregated_importances = np.zeros(len(predictors))\n",
    "    \n",
    "    # Compute the mean importances for this month's models\n",
    "    importances = np.array([model.feature_importances_ for model in xgboost_models])\n",
    "    mean_importances = np.mean(importances, axis=0)\n",
    "    \n",
    "    # Sorting indices for plotting (highest to lowest)\n",
    "    sorted_indices = np.argsort(mean_importances)[::-1]\n",
    "    \n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(np.array(predictors)[sorted_indices], mean_importances[sorted_indices], color='skyblue')\n",
    "    plt.xlabel('Average Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Average Feature Importance for {month}')\n",
    "    plt.gca().invert_yaxis()  # Show most important feature at the top\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to specified directory\n",
    "    plt.savefig(f'{output_dir}/Aggregated_Feature_Importances_{month}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Initialise DataFrames to store explained variance and metrics\n",
    "explained_variance_df = pd.DataFrame(columns=['Date', 'Explained Variance XGBoost', 'Explained Variance LSTM', 'Explained Variance NN', 'Explained Variance Ensemble'])\n",
    "metrics_per_country_list = []\n",
    "\n",
    "# Define prediction dates\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']  \n",
    "\n",
    "# TimeSeriesSplit object\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Loop through each prediction date and generate forecasts\n",
    "for date in prediction_dates:\n",
    "    print(f\"Processing predictions for {date}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_data, val_data = split_train_val(data_agg, quarter='Q4')\n",
    "    test_data = data_agg[data_agg['year_month'] == date]\n",
    "    \n",
    "    X_train = train_data[predictors]\n",
    "    y_train = train_data[target_var]\n",
    "    X_val = val_data[predictors]\n",
    "    y_val = val_data[target_var]\n",
    "    X_test = test_data[predictors]\n",
    "    y_test = test_data[target_var]\n",
    "    \n",
    "    # Perform walk-forward validation\n",
    "    lstm_history = []\n",
    "    nn_history = []\n",
    "    xgboost_models = []\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    # Initialise lists to store RMSE values\n",
    "    lstm_rmse_train = []\n",
    "    nn_rmse_train = []\n",
    "    xgboost_rmse_train = []\n",
    "\n",
    "    lstm_rmse_val = []\n",
    "    nn_rmse_val = []\n",
    "    xgboost_rmse_val = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X_train_np):\n",
    "        X_train_cv, X_val_cv = X_train_np[train_index], X_train_np[val_index]\n",
    "        y_train_cv, y_val_cv = y_train_np[train_index], y_train_np[val_index]\n",
    "        \n",
    "        X_train_lstm = np.reshape(X_train_cv, (X_train_cv.shape[0], 1, X_train_cv.shape[1]))\n",
    "        X_val_lstm = np.reshape(X_val_cv, (X_val_cv.shape[0], 1, X_val_cv.shape[1]))\n",
    "        \n",
    "        num_models = 5\n",
    "        \n",
    "        lstm_models = []\n",
    "        for i in range(num_models):\n",
    "            # Set a new random seed for each model based on the loop index\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "            model, history = create_and_train_lstm_with_attention(X_train_lstm, y_train_cv, X_val_lstm, y_val_cv)\n",
    "            lstm_models.append(model)\n",
    "            lstm_history.append(history)\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_lstm = model.predict(X_train_lstm).flatten()\n",
    "            val_pred_lstm = model.predict(X_val_lstm).flatten()\n",
    "\n",
    "            lstm_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_lstm)))\n",
    "            lstm_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_lstm)))\n",
    "        \n",
    "        nn_models = []\n",
    "        for i in range(num_models):\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            \n",
    "            model, history = create_best_nn_model(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "            nn_models.append(model)\n",
    "            nn_history.append(history)\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_nn = model.predict(X_train_cv).flatten()\n",
    "            val_pred_nn = model.predict(X_val_cv).flatten()\n",
    "\n",
    "            nn_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_nn)))\n",
    "            nn_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_nn)))\n",
    "\n",
    "        xgboost_models_this_month = []\n",
    "        for i in range(num_models):\n",
    "            seed = random_seed + i\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            random.seed(seed)\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            \n",
    "            model = train_xgboost_model(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "            xgboost_models_this_month.append(model)\n",
    "\n",
    "            # Compute RMSE for train and validation sets\n",
    "            train_pred_xgb = model.predict(X_train_cv)\n",
    "            val_pred_xgb = model.predict(X_val_cv)\n",
    "\n",
    "            xgboost_rmse_train.append(np.sqrt(mean_squared_error(y_train_cv, train_pred_xgb)))\n",
    "            xgboost_rmse_val.append(np.sqrt(mean_squared_error(y_val_cv, val_pred_xgb)))\n",
    "\n",
    "        xgboost_models.append(xgboost_models_this_month)\n",
    "\n",
    "    # Calculate average RMSE over all models\n",
    "    avg_lstm_rmse_train = np.mean(lstm_rmse_train)\n",
    "    avg_nn_rmse_train = np.mean(nn_rmse_train)\n",
    "    avg_xgboost_rmse_train = np.mean(xgboost_rmse_train)\n",
    "\n",
    "    avg_lstm_rmse_val = np.mean(lstm_rmse_val)\n",
    "    avg_nn_rmse_val = np.mean(nn_rmse_val)\n",
    "    avg_xgboost_rmse_val = np.mean(xgboost_rmse_val)\n",
    "\n",
    "    # Calculate weights based on the average RMSE\n",
    "    train_weights = calculate_weights([avg_xgboost_rmse_train, avg_lstm_rmse_train, avg_nn_rmse_train])\n",
    "    val_weights = calculate_weights([avg_xgboost_rmse_val, avg_lstm_rmse_val, avg_nn_rmse_val])\n",
    "\n",
    "    # Generate bootstrapped predictions for each model\n",
    "    lstm_bootstrap_preds = bootstrap_predictions(\n",
    "        create_and_train_lstm_with_attention, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "    nn_bootstrap_preds = bootstrap_predictions(\n",
    "        create_best_nn_model, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "    xgboost_bootstrap_preds = bootstrap_predictions(\n",
    "        train_xgboost_model, X_train_np, y_train_np, X_val_np, y_val_np, X_test_np\n",
    "    )\n",
    "\n",
    "    # Calculate CI widths and choose the model with the widest CI for ensemble predictions\n",
    "    lstm_mean, lstm_lower_ci, lstm_upper_ci = calculate_confidence_intervals(lstm_bootstrap_preds)\n",
    "    nn_mean, nn_lower_ci, nn_upper_ci = calculate_confidence_intervals(nn_bootstrap_preds)\n",
    "    xgboost_mean, xgboost_lower_ci, xgboost_upper_ci = calculate_confidence_intervals(xgboost_bootstrap_preds)\n",
    "\n",
    "    lstm_ci_width = lstm_upper_ci - lstm_lower_ci\n",
    "    nn_ci_width = nn_upper_ci - nn_lower_ci\n",
    "    xgboost_ci_width = xgboost_upper_ci - xgboost_lower_ci\n",
    "\n",
    "    # Create an array to hold ensemble predictions using the model with the widest CI\n",
    "    ensemble_mean_widest_ci = np.zeros_like(lstm_mean)\n",
    "    ensemble_lower_ci = np.zeros_like(lstm_lower_ci)  # Define these arrays to store ensemble CI data\n",
    "    ensemble_upper_ci = np.zeros_like(lstm_upper_ci)\n",
    "\n",
    "    # Selecting the model with the widest CI for each instance\n",
    "    for i in range(len(ensemble_mean_widest_ci)):\n",
    "        if lstm_ci_width[i] >= nn_ci_width[i] and lstm_ci_width[i] >= xgboost_ci_width[i]:\n",
    "            ensemble_mean_widest_ci[i] = lstm_mean[i]\n",
    "            ensemble_lower_ci[i] = lstm_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = lstm_upper_ci[i]\n",
    "        elif nn_ci_width[i] >= lstm_ci_width[i] and nn_ci_width[i] >= xgboost_ci_width[i]:\n",
    "            ensemble_mean_widest_ci[i] = nn_mean[i]\n",
    "            ensemble_lower_ci[i] = nn_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = nn_upper_ci[i]\n",
    "        else:\n",
    "            ensemble_mean_widest_ci[i] = xgboost_mean[i]\n",
    "            ensemble_lower_ci[i] = xgboost_lower_ci[i]\n",
    "            ensemble_upper_ci[i] = xgboost_upper_ci[i]\n",
    "\n",
    "    # Rescale predictions\n",
    "    y_test_rescaled = scaler_y.inverse_transform(y_test_np.reshape(-1, 1)).flatten()\n",
    "    ensemble_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_mean_widest_ci.reshape(-1, 1)).flatten())\n",
    "\n",
    "    results_df = test_data[['country', 'year_month']].copy()\n",
    "    results_df['True Values'] = np.round(y_test_rescaled[:results_df.shape[0]])\n",
    "    results_df['Ensemble Prediction'] = np.round(ensemble_pred_rescaled[:results_df.shape[0]])\n",
    "\n",
    "    # Add lower and upper CIs for each model and the ensemble\n",
    "    results_df['XGBoost Lower CI'] = np.round(scaler_y.inverse_transform(xgboost_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['XGBoost Upper CI'] = np.round(scaler_y.inverse_transform(xgboost_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['LSTM Lower CI'] = np.round(scaler_y.inverse_transform(lstm_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['LSTM Upper CI'] = np.round(scaler_y.inverse_transform(lstm_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['NN Lower CI'] = np.round(scaler_y.inverse_transform(nn_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['NN Upper CI'] = np.round(scaler_y.inverse_transform(nn_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    \n",
    "    results_df['Ensemble Lower CI'] = np.round(scaler_y.inverse_transform(ensemble_lower_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['Ensemble Upper CI'] = np.round(scaler_y.inverse_transform(ensemble_upper_ci.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "\n",
    "    results_df['Residuals'] = results_df['True Values'] - results_df['Ensemble Prediction']\n",
    "\n",
    "    # Store individual model predictions\n",
    "    results_df['XGBoost Prediction'] = np.round(scaler_y.inverse_transform(xgboost_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['LSTM Prediction'] = np.round(scaler_y.inverse_transform(lstm_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    results_df['NN Prediction'] = np.round(scaler_y.inverse_transform(nn_mean.reshape(-1, 1)).flatten()[:results_df.shape[0]])\n",
    "    \n",
    "    # Calculate residuals for training and validation sets\n",
    "    train_pred_xgb_list = [model.predict(X_train_np) for model in xgboost_models_this_month]\n",
    "    train_pred_xgb = np.mean(train_pred_xgb_list, axis=0)\n",
    "    \n",
    "    train_pred_lstm_list = [model.predict(np.reshape(X_train_np, (X_train_np.shape[0], 1, X_train_np.shape[1]))).flatten() for model in lstm_models]\n",
    "    train_pred_lstm = np.mean(train_pred_lstm_list, axis=0)\n",
    "    \n",
    "    train_pred_nn_list = [model.predict(X_train_np).flatten() for model in nn_models]\n",
    "    train_pred_nn = np.mean(train_pred_nn_list, axis=0)\n",
    "\n",
    "    # Compute weighted average for train predictions\n",
    "    ensemble_train_pred = (train_weights[0] * train_pred_xgb +\n",
    "                           train_weights[1] * train_pred_lstm +\n",
    "                           train_weights[2] * train_pred_nn)\n",
    "\n",
    "    # Rescale train predictions\n",
    "    train_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_train_pred.reshape(-1, 1)).flatten())\n",
    "    y_train_rescaled = scaler_y.inverse_transform(y_train_np.reshape(-1, 1)).flatten()\n",
    "    train_residuals = y_train_rescaled - train_pred_rescaled\n",
    "    train_data['Residuals'] = train_residuals\n",
    "\n",
    "    # Calculate residuals for validation set\n",
    "    val_pred_xgb_list = [model.predict(X_val_np) for model in xgboost_models_this_month]\n",
    "    val_pred_xgb = np.mean(val_pred_xgb_list, axis=0)\n",
    "\n",
    "    val_pred_lstm_list = [model.predict(np.reshape(X_val_np, (X_val_np.shape[0], 1, X_val_np.shape[1]))).flatten() for model in lstm_models]\n",
    "    val_pred_lstm = np.mean(val_pred_lstm_list, axis=0)\n",
    "    \n",
    "    val_pred_nn_list = [model.predict(X_val_np).flatten() for model in nn_models]\n",
    "    val_pred_nn = np.mean(val_pred_nn_list, axis=0)\n",
    "\n",
    "    # Compute weighted average for validation predictions\n",
    "    ensemble_val_pred = (val_weights[0] * val_pred_xgb +\n",
    "                         val_weights[1] * val_pred_lstm +\n",
    "                         val_weights[2] * val_pred_nn)\n",
    "\n",
    "    val_pred_rescaled = np.round(scaler_y.inverse_transform(ensemble_val_pred.reshape(-1, 1)).flatten())\n",
    "    y_val_rescaled = scaler_y.inverse_transform(y_val_np.reshape(-1, 1)).flatten()\n",
    "    val_residuals = y_val_rescaled - val_pred_rescaled\n",
    "    val_data['Residuals'] = val_residuals\n",
    "\n",
    "    # Append residuals to the all_residuals DataFrame\n",
    "    test_residuals = results_df['True Values'] - results_df['Ensemble Prediction']\n",
    "    test_data['Residuals'] = test_residuals\n",
    "    \n",
    "    # Add a column to identify data splits\n",
    "    train_data['data_split'] = 'train'\n",
    "    val_data['data_split'] = 'val'\n",
    "    test_data['data_split'] = 'test'\n",
    "\n",
    "    # Initialise DataFrame to store all residuals\n",
    "    all_residuals = pd.DataFrame()\n",
    "\n",
    "    # Concatenate train, validation, and test data\n",
    "    all_residuals = pd.concat([\n",
    "        all_residuals,\n",
    "        train_data[['country', 'year_month', 'Residuals', 'data_split']],\n",
    "        val_data[['country', 'year_month', 'Residuals', 'data_split']],\n",
    "        test_data[['country', 'year_month', 'Residuals', 'data_split']]\n",
    "    ])\n",
    "\n",
    "    groups = results_df.groupby(['country'])\n",
    "    for country, group_data in groups:\n",
    "        print(f\"Residuals for Country: {country}\")\n",
    "        print(group_data[['year_month', 'Residuals']])\n",
    "\n",
    "    # Calculate metrics for each country\n",
    "    metrics_per_country = []\n",
    "    for country, group_data in groups:\n",
    "        y_true_country = group_data['True Values'].values\n",
    "        y_pred_country = group_data['Ensemble Prediction'].values\n",
    "        y_pred_xgb_country = group_data['XGBoost Prediction'].values\n",
    "        y_pred_lstm_country = group_data['LSTM Prediction'].values\n",
    "        y_pred_nn_country = group_data['NN Prediction'].values\n",
    "        \n",
    "        if not np.any(np.isnan(y_true_country)) and not np.any(np.isnan(y_pred_country)):\n",
    "            mse_country = mean_squared_error(y_true_country, y_pred_country)\n",
    "            rmse_country = np.sqrt(mse_country)  \n",
    "            mae_country = mean_absolute_error(y_true_country, y_pred_country)\n",
    "            mdae_country = median_absolute_error(y_true_country, y_pred_country)\n",
    "            mape_country = mean_absolute_percentage_error(y_true_country + 1e-6, y_pred_country)\n",
    "\n",
    "            mse_xgb_country = mean_squared_error(y_true_country, y_pred_xgb_country)\n",
    "            rmse_xgb_country = np.sqrt(mse_xgb_country)\n",
    "            mae_xgb_country = mean_absolute_error(y_true_country, y_pred_xgb_country)\n",
    "            mdae_xgb_country = median_absolute_error(y_true_country, y_pred_xgb_country)\n",
    "            mape_xgb_country = mean_absolute_percentage_error(y_true_country + 1e-6, y_pred_xgb_country)\n",
    "\n",
    "            mse_lstm_country = mean_squared_error(y_true_country, y_pred_lstm_country)\n",
    "            rmse_lstm_country = np.sqrt(mse_lstm_country)\n",
    "            mae_lstm_country = mean_absolute_error(y_true_country, y_pred_lstm_country)\n",
    "            mdae_lstm_country = median_absolute_error(y_true_country, y_pred_lstm_country)\n",
    "            mape_lstm_country = mean_absolute_percentage_error(y_true_country + 1e-6, y_pred_lstm_country)\n",
    "\n",
    "            mse_nn_country = mean_squared_error(y_true_country, y_pred_nn_country)\n",
    "            rmse_nn_country = np.sqrt(mse_nn_country)\n",
    "            mae_nn_country = mean_absolute_error(y_true_country, y_pred_nn_country)\n",
    "            mdae_nn_country = median_absolute_error(y_true_country, y_pred_nn_country)\n",
    "            mape_nn_country = mean_absolute_percentage_error(y_true_country + 1e-6, y_pred_nn_country)\n",
    "            \n",
    "            metrics_per_country.append({\n",
    "                'Country': country,\n",
    "                'MSE Ensemble': np.round(mse_country),\n",
    "                'RMSE Ensemble': np.round(rmse_country),\n",
    "                'MAE Ensemble': np.round(mae_country),\n",
    "                'MDAE Ensemble': np.round(mdae_country),\n",
    "                'MAPE Ensemble': np.round(mape_country),\n",
    "                'MSE XGBoost': np.round(mse_xgb_country),\n",
    "                'RMSE XGBoost': np.round(rmse_xgb_country),\n",
    "                'MAE XGBoost': np.round(mae_xgb_country),\n",
    "                'MDAE XGBoost': np.round(mdae_xgb_country),\n",
    "                'MAPE XGBoost': np.round(mape_xgb_country),\n",
    "                'MSE LSTM': np.round(mse_lstm_country),\n",
    "                'RMSE LSTM': np.round(rmse_lstm_country),\n",
    "                'MAE LSTM': np.round(mae_lstm_country),\n",
    "                'MDAE LSTM': np.round(mdae_lstm_country),\n",
    "                'MAPE LSTM': np.round(mape_lstm_country),\n",
    "                'MSE NN': np.round(mse_nn_country),\n",
    "                'RMSE NN': np.round(rmse_nn_country),\n",
    "                'MAE NN': np.round(mae_nn_country),\n",
    "                'MDAE NN': np.round(mdae_nn_country),\n",
    "                'MAPE NN': np.round(mape_nn_country)\n",
    "            })\n",
    "\n",
    "    metrics_country_df = pd.DataFrame(metrics_per_country)\n",
    "    metrics_per_country_list.append(metrics_country_df)\n",
    "    \n",
    "    # Save country-level metrics to CSV\n",
    "    metrics_country_df.to_csv(f'{output_dir}/performance_metrics_country_{date}.csv', index=False)\n",
    "\n",
    "    # Aggregate to total by summing up the country-level metrics\n",
    "    overall_metrics = metrics_country_df.sum(numeric_only=True).to_dict()\n",
    "    overall_metrics_df = pd.DataFrame([overall_metrics])\n",
    "    overall_metrics_df.to_csv(f'{output_dir}/performance_metrics_overall_{date}.csv', index=False)\n",
    "\n",
    "    # Plot feature importances for each month\n",
    "    plot_aggregated_feature_importances(xgboost_models_this_month, predictors, output_dir, date)\n",
    "\n",
    "    # Calculate explained variance for XGBoost, LSTM, NN, and Ensemble\n",
    "    explained_variance_xgb = explained_variance_score(y_test_rescaled, results_df['XGBoost Prediction'].values)\n",
    "    explained_variance_lstm = explained_variance_score(y_test_rescaled, results_df['LSTM Prediction'].values)\n",
    "    explained_variance_nn = explained_variance_score(y_test_rescaled, results_df['NN Prediction'].values)\n",
    "    explained_variance_ensemble = explained_variance_score(y_test_rescaled, ensemble_pred_rescaled)\n",
    "\n",
    "    # Add explained variance to DataFrame\n",
    "    explained_variance_df = pd.concat([explained_variance_df, pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'Explained Variance XGBoost': [explained_variance_xgb],\n",
    "        'Explained Variance LSTM': [explained_variance_lstm],\n",
    "        'Explained Variance NN': [explained_variance_nn],\n",
    "        'Explained Variance Ensemble': [explained_variance_ensemble]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    print(f\"Explained Variance - XGBoost: {explained_variance_xgb:.4f}\")\n",
    "    print(f\"Explained Variance - LSTM: {explained_variance_lstm:.4f}\")\n",
    "    print(f\"Explained Variance - NN: {explained_variance_nn:.4f}\")\n",
    "    print(f\"Explained Variance - Ensemble: {explained_variance_ensemble:.4f}\")\n",
    "\n",
    "    # Aggregate true values and predicted values at the country and month level\n",
    "    country_month_aggregation = results_df.groupby(['country', 'year_month']).sum(numeric_only=True).reset_index()\n",
    "    country_month_aggregation.to_csv(f'{output_dir}/country_level_aggregation_{date}.csv', index=False)\n",
    "\n",
    "    # Aggregate true values and predicted values overall by summing up the country-month-level metrics\n",
    "    overall_aggregation = country_month_aggregation.sum(numeric_only=True).to_dict()\n",
    "    overall_aggregation_df = pd.DataFrame([overall_aggregation])\n",
    "    overall_aggregation_df.to_csv(f'{output_dir}/overall_aggregation_{date}.csv', index=False)\n",
    "\n",
    "# Save explained variance for each model per month\n",
    "explained_variance_df.to_csv(f'{output_dir}/explained_variance_by_model_per_month.csv', index=False)\n",
    "\n",
    "# Plot combined residuals over time for each group after all forecasts\n",
    "plot_combined_residuals(all_residuals)\n",
    "\n",
    "# Plot learning curves for LSTM models\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, history in enumerate(lstm_history):\n",
    "    plt.plot(history.history['loss'], label=f'LSTM Train {i+1}', linestyle='-')\n",
    "    plt.plot(history.history['val_loss'], label=f'LSTM Val {i+1}', linestyle='--')\n",
    "plt.title('Learning Curves for LSTM Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/LSTM_Learning_Curves.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot learning curves for NN models\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, history in enumerate(nn_history):\n",
    "    plt.plot(history.history['loss'], label=f'NN Train {i+1}', linestyle='-')\n",
    "    plt.plot(history.history['val_loss'], label=f'NN Val {i+1}', linestyle='--')\n",
    "plt.title('Learning Curves for Neural Network Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/NN_Learning_Curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of model weights based on inverse RMSE for each forecasted month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  XGBoost Weight  LSTM Weight  NN Weight\n",
      "0  2024-01-01        0.362220     0.371043   0.266737\n",
      "1  2024-02-01        0.417606     0.303097   0.279296\n",
      "2  2024-03-01        0.588145     0.168127   0.243728\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the monthly performance metric files\n",
    "directory = 'Aggregated Approach Outputs'  \n",
    "files = sorted([f for f in os.listdir(directory) if f.startswith('performance_metrics_overall_')])\n",
    "\n",
    "# Initialise a list to store the results\n",
    "monthly_weights = []\n",
    "\n",
    "# Iterate over each file\n",
    "for file in files:\n",
    "    date_str = file.replace('performance_metrics_overall_', '').replace('.csv', '')\n",
    "    \n",
    "    # Load the performance metrics file\n",
    "    file_path = os.path.join(directory, file)\n",
    "    metrics_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the RMSE values for each model\n",
    "    rmse_xgboost = metrics_df['RMSE XGBoost'].values[0]\n",
    "    rmse_lstm = metrics_df['RMSE LSTM'].values[0]\n",
    "    rmse_nn = metrics_df['RMSE NN'].values[0]\n",
    "\n",
    "    # Calculate the inverse RMSE and then normalize to get the weights\n",
    "    inv_rmse_xgboost = 1 / rmse_xgboost\n",
    "    inv_rmse_lstm = 1 / rmse_lstm\n",
    "    inv_rmse_nn = 1 / rmse_nn\n",
    "\n",
    "    # Sum of inverse RMSE values\n",
    "    sum_inv_rmse = inv_rmse_xgboost + inv_rmse_lstm + inv_rmse_nn\n",
    "\n",
    "    # Calculate the weights\n",
    "    weight_xgboost = inv_rmse_xgboost / sum_inv_rmse\n",
    "    weight_lstm = inv_rmse_lstm / sum_inv_rmse\n",
    "    weight_nn = inv_rmse_nn / sum_inv_rmse\n",
    "\n",
    "    # Append the results to the list\n",
    "    monthly_weights.append({\n",
    "        'Date': date_str,\n",
    "        'XGBoost Weight': weight_xgboost,\n",
    "        'LSTM Weight': weight_lstm,\n",
    "        'NN Weight': weight_nn\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame for easy viewing and saving\n",
    "weights_df = pd.DataFrame(monthly_weights)\n",
    "\n",
    "# Display the weights per month\n",
    "print(weights_df)\n",
    "\n",
    "# Save the weights to a CSV file\n",
    "weights_df.to_csv('Aggregated Approach Outputs/model_weights_per_month.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of correlation between model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The predictions from the XGBoost, LSTM, and Neural Network (NN) models are averaged across all bootstrapped predictions.\n",
    "# This averaging is performed along the axis 0, which corresponds to the first axis (i.e., rows).\n",
    "xgb_predictions = np.mean(xgboost_bootstrap_preds, axis=0)\n",
    "lstm_predictions = np.mean(lstm_bootstrap_preds, axis=0)\n",
    "nn_predictions = np.mean(nn_bootstrap_preds, axis=0)\n",
    "\n",
    "# The averaged predictions from each model are combined into a single DataFrame.\n",
    "# Each column in the DataFrame corresponds to the predictions from one model: XGBoost, LSTM, and NN.\n",
    "predictions_df = pd.DataFrame({\n",
    "    'XGBoost': xgb_predictions,\n",
    "    'LSTM': lstm_predictions,\n",
    "    'NN': nn_predictions\n",
    "})\n",
    "\n",
    "# A correlation matrix is calculated to understand the relationships between the predictions of different models.\n",
    "# The correlation matrix quantifies how similar the predictions from different models are to each other.\n",
    "correlation_matrix = predictions_df.corr()\n",
    "\n",
    "# The calculated correlation matrix is printed to the console.\n",
    "# This provides an immediate view of the degree to which the models' predictions are correlated.\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics across demographic groups per country of citizenship, and across countries for overall asylum applications submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Define the output directory where the results are saved\n",
    "output_dir = \"Aggregated Approach Outputs\"\n",
    "\n",
    "# Define the prediction dates used in the main code\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']  \n",
    "\n",
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE), Root Mean Squared Error (RMSE),\n",
    "    Mean Absolute Error (MAE), and Median Absolute Error (MDAE).\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mdae = median_absolute_error(y_true, y_pred)\n",
    "    return mse, rmse, mae, mdae\n",
    "\n",
    "# Iterate through each prediction date to load and process the corresponding result files\n",
    "for date in prediction_dates:\n",
    "    # Load the results file for the current prediction date\n",
    "    results_df = pd.read_csv(f'{output_dir}/country_level_aggregation_{date}.csv')\n",
    "\n",
    "    # Extract true values and predictions\n",
    "    y_true = results_df['True Values'].values\n",
    "    y_pred_ensemble = results_df['Ensemble Prediction'].values\n",
    "    y_pred_xgb = results_df['XGBoost Prediction'].values\n",
    "    y_pred_lstm = results_df['LSTM Prediction'].values\n",
    "    y_pred_nn = results_df['NN Prediction'].values\n",
    "\n",
    "    # Calculate overall performance metrics across all countries for the current month\n",
    "    overall_mse_ensemble, overall_rmse_ensemble, overall_mae_ensemble, overall_mdae_ensemble = calculate_metrics(y_true, y_pred_ensemble)\n",
    "    overall_mse_xgb, overall_rmse_xgb, overall_mae_xgb, overall_mdae_xgb = calculate_metrics(y_true, y_pred_xgb)\n",
    "    overall_mse_lstm, overall_rmse_lstm, overall_mae_lstm, overall_mdae_lstm = calculate_metrics(y_true, y_pred_lstm)\n",
    "    overall_mse_nn, overall_rmse_nn, overall_mae_nn, overall_mdae_nn = calculate_metrics(y_true, y_pred_nn)\n",
    "\n",
    "    # Store overall metrics across all countries in a DataFrame for the current month\n",
    "    overall_metrics_df = pd.DataFrame({\n",
    "        'Model': ['Ensemble', 'XGBoost', 'LSTM', 'NN'],\n",
    "        'MSE': [overall_mse_ensemble, overall_mse_xgb, overall_mse_lstm, overall_mse_nn],\n",
    "        'RMSE': [overall_rmse_ensemble, overall_rmse_xgb, overall_rmse_lstm, overall_rmse_nn],\n",
    "        'MAE': [overall_mae_ensemble, overall_mae_xgb, overall_mae_lstm, overall_mae_nn],\n",
    "        'MDAE': [overall_mdae_ensemble, overall_mdae_xgb, overall_mdae_lstm, overall_mdae_nn]\n",
    "    }).round(2)\n",
    "\n",
    "    # Save the overall performance metrics across all countries to a CSV file for the current month\n",
    "    overall_metrics_df.to_csv(f'{output_dir}/overall_performance_metrics_{date}.csv', index=False)\n",
    "\n",
    "    # Display the results for the current month\n",
    "    print(f\"Overall Performance Metrics Across All Countries for {date}:\")\n",
    "    print(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble prediction errors across countries of citizenship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the output directory where the results are saved\n",
    "output_dir = \"Aggregated Approach Outputs\"\n",
    "\n",
    "# Define the prediction dates used in the main code\n",
    "prediction_dates = ['2024-01-01', '2024-02-01', '2024-03-01']  \n",
    "\n",
    "# Initialise a DataFrame to hold overall errors across all countries\n",
    "overall_errors = pd.DataFrame()\n",
    "\n",
    "# Process each prediction date\n",
    "for date in prediction_dates:\n",
    "    # Load the aggregation results file for the current prediction date\n",
    "    filepath = f'{output_dir}/country_level_aggregation_{date}.csv'\n",
    "    if os.path.exists(filepath):\n",
    "        data_df = pd.read_csv(filepath)\n",
    "\n",
    "        # Calculate the error between True Values and Ensemble Prediction\n",
    "        data_df['Ensemble Error'] = data_df['True Values'] - data_df['Ensemble Prediction']\n",
    "\n",
    "        # Save the updated DataFrame with the Error column back to CSV\n",
    "        data_df.to_csv(f'{output_dir}/country_level_aggregation_with_error_{date}.csv', index=False)\n",
    "        \n",
    "        # Append to the overall errors DataFrame\n",
    "        overall_errors = pd.concat([overall_errors, data_df], ignore_index=True)\n",
    "\n",
    "# Summarise errors by country and date\n",
    "country_errors = overall_errors.groupby(['country', 'year_month'])['Ensemble Error'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by 'year_month' and then by 'country'\n",
    "country_errors = country_errors.sort_values(by=['year_month', 'country'])\n",
    "\n",
    "# Save the sorted errors by country and date to a CSV file\n",
    "country_errors.to_csv(f'{output_dir}/errors_by_country_and_date.csv', index=False)\n",
    "\n",
    "# Calculate overall error across all countries and dates\n",
    "total_error = overall_errors['Ensemble Error'].sum()\n",
    "print(f\"Total Error across all countries and dates: {total_error}\")\n",
    "\n",
    "# Save this overall error to a CSV for documentation\n",
    "overall_error_df = pd.DataFrame({'Total Error': [total_error]})\n",
    "overall_error_df.to_csv(f'{output_dir}/total_error_across_all_countries.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Errors by country and date:\")\n",
    "print(country_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising asylum application forecasts with uncertainty intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# Set the font to Times New Roman globally for all plots\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12  \n",
    "\n",
    "# Load the aggregated data from the final thesis dataset\n",
    "data_agg = pd.read_csv('final_thesis_data.csv')\n",
    "\n",
    "# Create an output directory for saving graphs\n",
    "output_dir = \"GHVT6_Outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the 'year_month' column is in datetime format\n",
    "data_agg['year_month'] = pd.to_datetime(data_agg['year_month'])\n",
    "\n",
    "# Aggregate data at the country level by summing asylum applications for each country per month\n",
    "country_data_agg = data_agg.groupby(['country', 'year_month']).agg({'asy_applications': 'sum'}).reset_index()\n",
    "\n",
    "# Define the forecast dates and the corresponding forecast files\n",
    "forecast_files = {\n",
    "    '2024-01-01': 'Aggregated Approach Outputs/country_level_aggregation_2024-01-01.csv',\n",
    "    '2024-02-01': 'Aggregated Approach Outputs/country_level_aggregation_2024-02-01.csv',\n",
    "    '2024-03-01': 'Aggregated Approach Outputs/country_level_aggregation_2024-03-01.csv'\n",
    "}\n",
    "all_results_df = country_data_agg.copy()\n",
    "\n",
    "# Loop through each forecast file, merging forecast data with the aggregated country data\n",
    "for date, forecast_file in forecast_files.items():\n",
    "    if os.path.exists(forecast_file):\n",
    "        forecast_df = pd.read_csv(forecast_file)\n",
    "        forecast_df['year_month'] = pd.to_datetime(date)  # Assign the forecast date\n",
    "        forecast_df.rename(columns={\n",
    "            'Ensemble Prediction': f'Ensemble Prediction_{date}',\n",
    "            'Ensemble Lower CI': f'Lower CI_{date}',\n",
    "            'Ensemble Upper CI': f'Upper CI_{date}'\n",
    "        }, inplace=True)\n",
    "        all_results_df = pd.merge(\n",
    "            all_results_df,\n",
    "            forecast_df[['country', 'year_month', f'Ensemble Prediction_{date}', f'Lower CI_{date}', f'Upper CI_{date}']],\n",
    "            on=['country', 'year_month'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Forecast file for {date} not found.\")\n",
    "\n",
    "# Save the merged data to a CSV file for further inspection\n",
    "merged_data_filepath = os.path.join(output_dir, 'uncertainty_segmented_approach_country_level.csv')\n",
    "all_results_df.to_csv(merged_data_filepath, index=False)\n",
    "print(f\"Merged data saved to {merged_data_filepath}\")\n",
    "\n",
    "# Create a directory for saving individual plots\n",
    "individual_plots_dir = os.path.join(output_dir, \"Forecasts' Plots with Uncertainty - Aggregated Forecasting Approach\")\n",
    "os.makedirs(individual_plots_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each country to generate and save forecast plots\n",
    "for country, country_data in all_results_df.groupby('country'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values of asylum applications over time\n",
    "    ax.plot(country_data['year_month'], country_data['asy_applications'], label='True Values', linestyle='-', color='blue')\n",
    "    \n",
    "    # Plot the forecasted values and confidence intervals if forecast data is available\n",
    "    forecast_mask = country_data['year_month'].isin(pd.to_datetime(list(forecast_files.keys())))\n",
    "    if forecast_mask.any():\n",
    "        ax.plot(country_data['year_month'], \n",
    "                country_data[['Ensemble Prediction_2024-01-01', 'Ensemble Prediction_2024-02-01', 'Ensemble Prediction_2024-03-01']]\n",
    "                .ffill(axis=1).bfill(axis=1).iloc[:, -1], \n",
    "                label='Predictions', linestyle='--', color='red', marker=None)\n",
    "        ax.fill_between(country_data['year_month'], \n",
    "                        country_data[['Lower CI_2024-01-01', 'Lower CI_2024-02-01', 'Lower CI_2024-03-01']].min(axis=1), \n",
    "                        country_data[['Upper CI_2024-01-01', 'Upper CI_2024-02-01', 'Upper CI_2024-03-01']].max(axis=1), \n",
    "                        alpha=0.2, color='red', label='95% CI')\n",
    "    \n",
    "    ax.set_title(f'Forecast for {country}')\n",
    "    ax.set_xlabel('Year Month')\n",
    "    ax.set_ylabel('Asylum Applications')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Format the x-axis for date presentation\n",
    "    ax_inset = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper center', borderpad=2)\n",
    "    ax_inset.plot(country_data['year_month'], country_data['asy_applications'], linestyle='-', color='blue')\n",
    "    if forecast_mask.any():\n",
    "        ax_inset.plot(country_data['year_month'], \n",
    "                      country_data[['Ensemble Prediction_2024-01-01', 'Ensemble Prediction_2024-02-01', 'Ensemble Prediction_2024-03-01']]\n",
    "                      .ffill(axis=1).bfill(axis=1).iloc[:, -1], \n",
    "                      linestyle='--', color='red', marker=None)\n",
    "        ax_inset.fill_between(country_data['year_month'], \n",
    "                              country_data[['Lower CI_2024-01-01', 'Lower CI_2024-02-01', 'Lower CI_2024-03-01']].min(axis=1), \n",
    "                              country_data[['Upper CI_2024-01-01', 'Upper CI_2024-02-01', 'Upper CI_2024-03-01']].max(axis=1), \n",
    "                              alpha=0.5, color='red')\n",
    "    \n",
    "    # Zoom in on the forecast period in the inset\n",
    "    ax_inset.set_xlim(pd.to_datetime('2023-12-01'), pd.to_datetime('2024-03-31'))\n",
    "    ax_inset.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax_inset.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    # Save the plot for each country\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{individual_plots_dir}/Forecast_Comparison_{country}.png')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Plots saved in directory: {individual_plots_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
