{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fetch the list of files from the GDELT 2.0 project's server, filter for URLs ending in 'export.CSV.zip', print these URLs, and count and print the total number of such files available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import the requests library to handle HTTP requests\n",
    "\n",
    "# URL containing the list of files\n",
    "url = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "\n",
    "# Send a GET request to fetch the content of the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:  # status_code 200 indicates a successful response\n",
    "    # Split the content into lines\n",
    "    lines = response.text.splitlines()  # Split the response text into individual lines\n",
    "    \n",
    "    # Filter lines that end with 'export.CSV.zip'\n",
    "    csv_zip_urls = [line for line in lines if line.endswith('export.CSV.zip')]  \n",
    "    # Create a list of lines that end with 'export.CSV.zip', indicating URLs of interest\n",
    "\n",
    "    # Print or process the filtered URLs\n",
    "    for url in csv_zip_urls:\n",
    "        print(url.split()[-1])  # The URL is the last part after splitting by whitespace\n",
    "        # Split each line by whitespace and print the last part, which is the actual URL of the CSV.zip file\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)  # Print an error message if the request failed\n",
    "\n",
    "# Count the number of CSV.zip files available\n",
    "num_files = len(csv_zip_urls)\n",
    "print(f\"Number of available CSV.zip files: {num_files}\")  # Print the count of available CSV.zip files\n",
    "\n",
    "## Number of available CSV.zip files: 318544\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Fetch the list of files from the GDELT 2.0 project's server, filter for URLs ending in 'export.CSV.zip', download these files, and save them to a specified directory on the local machine, while also providing a progress count of the total and successfully downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import the requests library to handle HTTP requests\n",
    "import os  # Import the os library to handle file and directory operations\n",
    "\n",
    "# URL containing the list of files\n",
    "url = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "\n",
    "# Path to save the downloaded files, correctly expanding the home directory\n",
    "save_path = os.path.expanduser(\"~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# os.makedirs creates the specified directory, including any necessary but nonexistent parent directories.\n",
    "# The exist_ok parameter prevents raising an exception if the directory already exists.\n",
    "\n",
    "# Start the process\n",
    "print(\"Starting the download process...\")\n",
    "\n",
    "# Send a GET request to fetch the content of the URL\n",
    "response = requests.get(url)\n",
    "# This sends an HTTP GET request to the specified URL and stores the server's response in the response variable.\n",
    "\n",
    "if response.status_code == 200:  # Check if the request was successful (status code 200)\n",
    "    lines = response.text.splitlines()  # Split the response text into individual lines\n",
    "    csv_zip_urls = [line for line in lines if line.endswith('export.CSV.zip')]  \n",
    "    # Create a list of lines that end with 'export.CSV.zip', indicating URLs of interest\n",
    "    \n",
    "    total_files = len(csv_zip_urls)  # Get the total number of files to download\n",
    "    downloaded_files = 0  # Initialise a counter for the number of successfully downloaded files\n",
    "\n",
    "    for line in csv_zip_urls:\n",
    "        file_url = line.split()[-1]  # Extract the URL (the last part after splitting by whitespace)\n",
    "        filename = file_url.split('/')[-1]  # Extract the filename from the URL\n",
    "        file_path = os.path.join(save_path, filename)  # Create the full path for saving the file\n",
    "\n",
    "        print(f\"Downloading {file_url}...\")  # Print the URL of the file being downloaded\n",
    "        r = requests.get(file_url)  # Send a GET request to download the file\n",
    "        if r.status_code == 200:  # Check if the file was downloaded successfully\n",
    "            with open(file_path, 'wb') as f:  # Open the file in write-binary mode\n",
    "                f.write(r.content)  # Write the content of the response to the file\n",
    "            downloaded_files += 1  # Increment the counter of successfully downloaded files\n",
    "            print(f\"Saved to {file_path} ({downloaded_files}/{total_files})\")  \n",
    "            # Print a message indicating the file was saved and show the progress\n",
    "        else:\n",
    "            print(f\"Failed to download {file_url}. Status code: {r.status_code}\")  \n",
    "            # Print an error message if the file could not be downloaded\n",
    "\n",
    "    print(f\"Download process completed. {downloaded_files} out of {total_files} files were downloaded successfully.\")\n",
    "    # Print a completion message showing the number of files successfully downloaded\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "    # Print an error message if the initial request to fetch the list of files failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Process ZIP files containing CSV data, extract information from specific columns and save the filtered data to new CSV files in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import zipfile  # Import zipfile to handle zip files\n",
    "import os  # Import os to handle file and directory operations\n",
    "import csv  # Import csv to handle CSV file operations\n",
    "import logging  # Import logging to enable logging\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the directory containing your ZIP files and the output directory\n",
    "input_directory = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0')\n",
    "output_directory = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data')\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)  # Create the output directory \n",
    "\n",
    "batch_size = 10  # Adjust this based on how many files you want to process at a time\n",
    "\n",
    "def process_csv_entries(zip_file, filename):\n",
    " \n",
    "    ## Generator that yields selected data from a CSV file within a zip archive.\n",
    "    \n",
    "    ## Args:\n",
    "    ##    zip_file (ZipFile): The zipfile object containing the CSV file.\n",
    "    ##    filename (str): The name of the CSV file inside the zip archive.\n",
    "    \n",
    "    ## Yields:\n",
    "    ##     dict: A dictionary with selected data from the CSV file.\n",
    "   \n",
    "    with zip_file.open(filename, mode='r') as file:\n",
    "        reader = csv.reader((line.decode('utf-8') for line in file), delimiter='\\t')  \n",
    "        for row in reader:\n",
    "            if len(row) >= 58:  # Ensure the row has enough columns\n",
    "                yield {\n",
    "                    'year_month_day': row[1],\n",
    "                    'cameo_code': row[26],\n",
    "                    'goldstein_scale': row[30],\n",
    "                    'actor1_full_name': row[36],\n",
    "                    'actor1': row[37],\n",
    "                    'actor2_full_name': row[44],\n",
    "                    'actor2': row[45]\n",
    "                }\n",
    "\n",
    "def process_batch(zip_files):\n",
    "    \n",
    "    ## Process a batch of zip files and return a DataFrame of the extracted data.\n",
    "    \n",
    "    ## Args:\n",
    "    ##    zip_files (list): List of zip file names to process.\n",
    "    \n",
    "    ## Returns:\n",
    "    ##    DataFrame: A pandas DataFrame containing the extracted data.\n",
    "    \n",
    "    data = []\n",
    "    for zip_filename in zip_files:\n",
    "        zip_path = os.path.join(input_directory, zip_filename)\n",
    "        logging.info(f\"Processing ZIP file: {zip_path}\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for csv_filename in z.namelist():\n",
    "                logging.info(f\"Processing CSV file: {csv_filename}\")\n",
    "                entries = process_csv_entries(z, csv_filename)\n",
    "                data.extend(entries)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Process ZIP files in batches\n",
    "zip_files = [f for f in os.listdir(input_directory) if f.endswith('.zip')]\n",
    "for i in range(0, len(zip_files), batch_size):\n",
    "    batch_files = zip_files[i:i + batch_size]\n",
    "    logging.info(f\"Starting batch {i//batch_size + 1}/{(len(zip_files) + batch_size - 1)//batch_size}\")\n",
    "    df_batch = process_batch(batch_files)\n",
    "    if not df_batch.empty:\n",
    "        output_file_path = os.path.join(output_directory, f'filtered_data_batch_{i//batch_size + 1}.csv')\n",
    "        df_batch.to_csv(output_file_path, index=False)\n",
    "        logging.info(f\"Batch {i//batch_size + 1} saved successfully at {output_file_path}.\")\n",
    "    else:\n",
    "        logging.warning(f\"No data found in batch {i//batch_size + 1}.\")\n",
    "\n",
    "logging.info(\"Data extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Process CSV files to replace country initials with full names, filter rows based on specific countries, and save the filtered data to new CSV files while maintaining a log of processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import os to handle file and directory operations\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Define the directory with CSV files\n",
    "directory = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data')\n",
    "output_directory = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data/Processed GDELT 2.0 Data')  # Output directory for filtered files\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    # Create the output directory if it doesn't exist\n",
    "\n",
    "log_path = os.path.join(directory, 'processed_files.log')\n",
    "# Define the path to the log file that keeps track of processed files\n",
    "\n",
    "# Country initials and their mappings\n",
    "country_initials = {\n",
    "    'AF': 'Afghanistan', 'AL': 'Albania', 'BG': 'Bangladesh', 'CM': 'Cameroon',\n",
    "    'CG': 'Democratic Republic of the Congo', 'EG': 'Egypt', 'ER': 'Eritrea',\n",
    "    'GG': 'Georgia', 'IN': 'India', 'IR': 'Iran', 'IZ': 'Iraq', \n",
    "    'PK': 'Pakistan', 'WE': 'Palestine', 'GZ': 'Palestine', 'SL': 'Sierra Leone', 'SO': 'Somalia',\n",
    "    'SY': 'Syria', 'TU': 'Türkiye', 'YM': 'Yemen'\n",
    "}\n",
    "# Dictionary mapping country initials to their full names\n",
    "\n",
    "# Read in existing log entries, if any\n",
    "processed_files = set()\n",
    "if os.path.exists(log_path):\n",
    "    with open(log_path, 'r') as file:\n",
    "        processed_files.update(file.read().splitlines())\n",
    "        # Read previously processed files from the log and add them to the set\n",
    "\n",
    "# Process each file in the directory\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.csv') and f not in processed_files]\n",
    "# List all CSV files in the directory that haven't been processed yet\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing files\"):\n",
    "    # Iterate over the unprocessed CSV files with a progress bar\n",
    "    file_path = os.path.join(directory, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Read the CSV file into a DataFrame\n",
    "\n",
    "        # Replace country initials with full names\n",
    "        df['actor1'] = df['actor1'].replace(country_initials)\n",
    "        df['actor2'] = df['actor2'].replace(country_initials)\n",
    "\n",
    "        # Filter rows where either actor1 or actor2 is in the list of countries of interest\n",
    "        df_filtered = df[(df['actor1'].isin(country_initials.values())) | (df['actor2'].isin(country_initials.values()))]\n",
    "        \n",
    "        if not df_filtered.empty:\n",
    "            # If the filtered DataFrame is not empty, save it to a new CSV file in the output directory\n",
    "            interim_filename = f\"{os.path.splitext(file)[0]}_filtered.csv\"\n",
    "            interim_path = os.path.join(output_directory, interim_filename)\n",
    "            df_filtered.to_csv(interim_path, index=False)\n",
    "\n",
    "        # Log this file as processed\n",
    "        with open(log_path, 'a') as log_file:\n",
    "            log_file.write(file + '\\n')\n",
    "            # Append the file name to the log\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        # Print an error message if processing fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculate the Push Factor Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import os to handle file and directory operations\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "input_folder = '~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data/Processed GDELT 2.0 Data'\n",
    "output_folder = '~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data/Processed GDELT 2.0 Data/Processed_Outputs'\n",
    "os.makedirs(os.path.expanduser(output_folder), exist_ok=True)\n",
    "# Ensure the output folder exists, create it if it doesn't\n",
    "\n",
    "# Define the country codes and names for lookup\n",
    "country_initials = {\n",
    "    'AF': 'Afghanistan', 'AL': 'Albania', 'BG': 'Bangladesh', 'CM': 'Cameroon',\n",
    "    'CG': 'Democratic Republic of the Congo', 'EG': 'Egypt', 'ER': 'Eritrea',\n",
    "    'GG': 'Georgia', 'IN': 'India', 'IR': 'Iran', 'IZ': 'Iraq', \n",
    "    'PK': 'Pakistan', 'WE': 'Palestine', 'GZ': 'Palestine', 'SL': 'Sierra Leone', 'SO': 'Somalia',\n",
    "    'SY': 'Syria', 'TU': 'Türkiye', 'YM': 'Yemen'\n",
    "}\n",
    "# Dictionary mapping country initials to their full names\n",
    "\n",
    "# Reverse mapping to allow lookup by both code and full name\n",
    "country_lookup = {**country_initials, **{v: v for v in country_initials.values()}}\n",
    "\n",
    "# Define the CAMEO codes with their weights and categories\n",
    "events_info = {\n",
    "    '110': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "    '111': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "    '112': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "    '232': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '234': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '254': {'weight': 1, 'category': 'Economic', 'sign': '+'},\n",
    "    '255': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '25': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '20': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '27': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '28': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '354': {'weight': -1, 'category': 'Economic', 'sign': '-'},\n",
    "    '355': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '350': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '30': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '37': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '38': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '81': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '871': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '872': {'weight': -1, 'category': 'Conflict', 'sign': '-'},\n",
    "    '873': {'weight': -2, 'category': 'Conflict', 'sign': '-'},\n",
    "    '874': {'weight': -3, 'category': 'Conflict', 'sign': '-'},\n",
    "    '93': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '94': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1012': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1014': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1032': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1034': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1054': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '1055': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1050': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '100': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '107': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '108': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1123': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1124': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1125': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1244': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '1245': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1240': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '125': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '120': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '127': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '138': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1382': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1383': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1384': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1385': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '139': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '150': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '152': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '154': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '155': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '104': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '105': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '100': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '101': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '102': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '103': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '170': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '171': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '180': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '190': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '191': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "    '192': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '193': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '194': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '195': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1951': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '1952': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '190': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '200': {'weight': 4, 'category': 'Conflict', 'sign': '+'},\n",
    "    '204': {'weight': 4, 'category': 'Conflict', 'sign': '+'},\n",
    "    '2041': {'weight': 4, 'category': 'Conflict', 'sign': '+'},\n",
    "    '2042': {'weight': 4, 'category': 'Conflict', 'sign': '+'},\n",
    "    '23': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '231': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '1011': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '103': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '1031': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "    '24': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "    '241': {'weight': 1, 'category': 'Governance', 'sign': '+'}, \n",
    "\n",
    "'244': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'34': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'341': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'342': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'344': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'35': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'83': {'weight': -2, 'category': 'Governance', 'sign': '-'},\n",
    "'831': {'weight': -2, 'category': 'Governance', 'sign': '-'},\n",
    "'832': {'weight': -2, 'category': 'Governance', 'sign': '-'},\n",
    "'834': {'weight': -2, 'category': 'Governance', 'sign': '-'},\n",
    "'91': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'104': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1041': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1042': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1044': {'weight': -1, 'category': 'Governance', 'sign': '-'},\n",
    "'105': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1121': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'123': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1231': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1232': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1234': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'124': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'1241': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'128': {'weight': 1, 'category': 'Governance', 'sign': '+'},\n",
    "'243': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'251': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'253': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'343': {'weight': -1, 'category': 'Political', 'sign': '-'},\n",
    "'351': {'weight': -1, 'category': 'Political', 'sign': '-'},\n",
    "'353': {'weight': -1, 'category': 'Political', 'sign': '-'},\n",
    "'75': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "\n",
    "'811': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "'812': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "'813': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "'814': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "'833': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "'92': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1043': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1051': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1053': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1122': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1233': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1243': {'weight': 2, 'category': 'Political', 'sign': '+'},\n",
    "'1322': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1323': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'1324': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'137': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "'151': {'weight': 2, 'category': 'Political', 'sign': '+'},\n",
    "'153': {'weight': 2, 'category': 'Political', 'sign': '+'},\n",
    "'1711': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "\n",
    "'172': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1721': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1722': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1723': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1724': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'173': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'174': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'175': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'181': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1822': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'1823': {'weight': 3, 'category': 'Political', 'sign': '+'},\n",
    "'201': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "'202': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '203': {'weight': 3, 'category': 'Conflict', 'sign': '+'},\n",
    "    '233': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "    '252': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '352': {'weight': -1, 'category': 'Social', 'sign': '-'},\n",
    "    '82': {'weight': -2, 'category': 'Social', 'sign': '-'},\n",
    "    '84': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "    '841': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "    '842': {'weight': -2, 'category': 'Political', 'sign': '-'},\n",
    "    '1052': {'weight': 1, 'category': 'Political', 'sign': '+'},\n",
    "    '113': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "    '1242': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "    '133': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '1381': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '140': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '141': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '1411': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "    '1412': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '1413': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '1414': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "    '142': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "    '1421': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "    '1422': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "\n",
    "'1423': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1424': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'143': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1431': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'1432': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1433': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1434': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'144': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1441': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'1442': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1443': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'43': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1444': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'145': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'1451': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1452': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1453': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1454': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'171': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'1712': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'182': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'1821': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'183': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1831': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1832': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1833': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'1834': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'184': {'weight': 3, 'category': 'Social', 'sign': '+'},\n",
    "'185': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'180': {'weight': 2, 'category': 'Social', 'sign': '+'},\n",
    "'12': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'13': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'100': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'1': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'10': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'11': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'14': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'15': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'10': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'11': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'12': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "'121': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "'122': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'1221': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'1222': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'1223': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'1224': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'129': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'130': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'131': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1311': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1312': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1313': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'132': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1321': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'134': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'135': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'130': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'131': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1311': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1312': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1313': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'132': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'1321': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'134': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'135': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'130': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'14': {'weight': 1, 'category': 'Social', 'sign': '+'},\n",
    "'15': {'weight': 1, 'category': 'Conflict', 'sign': '+'},\n",
    "'101': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'102': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "'1021': {'weight': 2, 'category': 'Economic', 'sign': '+'},\n",
    "'1022': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "'1023': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "'103': {'weight': 2, 'category': 'Conflict', 'sign': '+'},\n",
    "'17': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'18': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'19': {'weight': 0, 'category': 'Other', 'sign': 'No'},\n",
    "'20': {'weight': 0, 'category': 'Other', 'sign': 'No'}\n",
    "\n",
    "}\n",
    "\n",
    "# Function to assign weights and categories\n",
    "def assign_event_info(cameo_code):\n",
    "    event = events_info.get(str(cameo_code), {'weight': 0, 'category': 'Unknown'})\n",
    "    return pd.Series([event['weight'], event['category']], index=['weight', 'category'])\n",
    "\n",
    "# Function to process each row and handle actors' country assignment\n",
    "def process_row(row):\n",
    "    actor1_country = country_lookup.get(row['actor1'], row['actor1'])\n",
    "    actor2_country = country_lookup.get(row['actor2'], row['actor2'])\n",
    "    row['country'] = actor1_country\n",
    "    \n",
    "    # Initialise the list of rows to return; always include the first row\n",
    "    rows = [row.copy()]\n",
    "    \n",
    "    # If actor2 is different from actor1, duplicate the row for actor2\n",
    "    if actor1_country != actor2_country:\n",
    "        new_row = row.copy()\n",
    "        new_row['country'] = actor2_country\n",
    "        rows.append(new_row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# Initialise an empty DataFrame to store the merged results\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Get the list of CSV files in the input folder\n",
    "csv_files = [f for f in os.listdir(os.path.expanduser(input_folder)) if f.endswith('.csv')]\n",
    "\n",
    "# Process each file\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(os.path.expanduser(input_folder), csv_file)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply the row processing function\n",
    "    expanded_rows = df.apply(process_row, axis=1)\n",
    "    new_df = pd.concat([pd.DataFrame(rows) for rows in expanded_rows], ignore_index=True)\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    new_df.drop(columns=['actor1', 'actor2', 'actor1_full_name', 'actor2_full_name'], inplace=True)\n",
    "    \n",
    "    # Drop duplicates if necessary and reset index\n",
    "    new_df.drop_duplicates(inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Apply function to assign weights and categories\n",
    "    new_df[['weight', 'category']] = new_df['cameo_code'].apply(assign_event_info)\n",
    "    \n",
    "    # Convert 'year_month_day' to datetime to facilitate resampling\n",
    "    new_df['year_month_day'] = pd.to_datetime(new_df['year_month_day'], format='%Y%m%d')\n",
    "    \n",
    "    # Set the datetime as the index\n",
    "    new_df.set_index('year_month_day', inplace=True)\n",
    "    \n",
    "    # Filter out rows with countries not in the specified list\n",
    "    valid_countries = list(country_codes.values()) + list(country_codes.keys())\n",
    "    new_df = new_df[new_df['country'].isin(valid_countries)]\n",
    "    \n",
    "    # Group by country and category, resample by month, and sum the weights\n",
    "    monthly_pfi = new_df.groupby(['country', 'category']).resample('M').sum()['weight'].unstack(level='category', fill_value=0)\n",
    "    \n",
    "    # Calculate the push_factor_index\n",
    "    monthly_pfi['push_factor_index'] = monthly_pfi.sum(axis=1)\n",
    "    \n",
    "    # Reset index to make 'country' and 'date' columns regular columns\n",
    "    monthly_pfi.reset_index(inplace=True)\n",
    "    \n",
    "    # Save the processed file to the output folder\n",
    "    output_file_path = os.path.join(os.path.expanduser(output_folder), f'processed_{csv_file}')\n",
    "    monthly_pfi.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    # Append to the merged DataFrame\n",
    "    merged_df = pd.concat([merged_df, monthly_pfi], ignore_index=True)\n",
    "    \n",
    "    # Save the merged results to a new CSV file after each file is processed\n",
    "    merged_merged_file_path = os.path.join(os.path.expanduser(output_folder), 'merged_pfi_by_country.csv')\n",
    "    merged_df.to_csv(merged_merged_file_path, index=False)\n",
    "\n",
    "print(\"Processing complete. Merged file saved as 'merged_pfi_by_country.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Process the final CSV file to convert date columns, count non-zero values in a specific column, determine unique countries, and identify any duplicate date entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import os  # Import os to handle file and directory operations\n",
    "\n",
    "# Define the directory and file path\n",
    "directory_path = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data/Processed GDELT 2.0 Data/Processed_Outputs')\n",
    "merged_file_path = os.path.join(directory_path, 'merged_pfi_by_country.csv')\n",
    "# Expand the user path and sets the path to the merged CSV file\n",
    "\n",
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv(merged_file_path)\n",
    "# Read the CSV file into a DataFrame\n",
    "\n",
    "# Convert 'year_month_day' to datetime \n",
    "merged_df['year_month_day'] = pd.to_datetime(merged_df['year_month_day'], format='%Y-%m-%d')\n",
    "# Convert the 'year_month_day' column to datetime format\n",
    "\n",
    "# Count nonzero values in the 'Other' column\n",
    "nonzero_count_other = (merged_df['Other'] != 0).sum()\n",
    "# Count the number of non-zero values in the 'Other' column\n",
    "\n",
    "# Count the number of unique countries\n",
    "unique_countries_count = merged_df['country'].nunique()\n",
    "# Count the number of unique countries in the 'country' column\n",
    "\n",
    "# Get the list of unique countries\n",
    "unique_countries = merged_df['country'].unique()\n",
    "# Retrieve the list of unique countries in the 'country' column\n",
    "\n",
    "# Find duplicate year_month_day values per country\n",
    "duplicate_counts = merged_df.groupby(['country', 'year_month_day']).size().reset_index(name='count')\n",
    "# Groups the DataFrame by 'country' and 'year_month_day', counts occurrences, and resets the index\n",
    "\n",
    "duplicates_per_country = duplicate_counts[duplicate_counts['count'] > 1]\n",
    "# Filter the grouped DataFrame to find duplicate entries (count > 1)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of nonzero values in the 'Other' column: {nonzero_count_other}\")\n",
    "print(f\"Number of unique countries in the dataset: {unique_countries_count}\")\n",
    "print(f\"Countries in the dataset: {unique_countries}\")\n",
    "\n",
    "if not duplicates_per_country.empty:\n",
    "    print(\"Duplicate year_month_day values per country:\")\n",
    "    print(duplicates_per_country)\n",
    "else:\n",
    "    print(\"No duplicate year_month_day values per country found.\")\n",
    "# Print the results, including non-zero counts, unique countries, and any duplicate date entries per country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Ensure data is aggregated to a monthly basis and save the results to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import os  # Import os to handle file and directory operations\n",
    "\n",
    "# Define the directory and file path\n",
    "directory_path = os.path.expanduser('~/Desktop/GHVT6_Thesis/GHVT6_Data/GDELT 2.0/Filtered GDELT 2.0 Data/Processed GDELT 2.0 Data/Processed_Outputs')\n",
    "merged_file_path = os.path.join(directory_path, 'merged_pfi_by_country.csv')\n",
    "# Expand the user path and sets the path to the merged CSV file\n",
    "\n",
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv(merged_file_path)\n",
    "# Read the CSV file into a DataFrame\n",
    "\n",
    "# Convert 'year_month_day' to datetime\n",
    "merged_df['year_month_day'] = pd.to_datetime(merged_df['year_month_day'], format='%Y-%m-%d')\n",
    "# Convert the 'year_month_day' column to datetime format\n",
    "\n",
    "# Extract year and month for aggregation\n",
    "merged_df['year_month'] = merged_df['year_month_day'].dt.to_period('M')\n",
    "# Extract the year and month from 'year_month_day' and creates a new 'year_month' column with monthly periods\n",
    "\n",
    "# Aggregate the data by summing up relevant columns for each 'country' and 'year_month'\n",
    "aggregated_columns = ['Conflict', 'Economic', 'Governance', 'Other', 'Political', 'Social', 'Unknown', 'push_factor_index']\n",
    "# Define the columns to aggregate\n",
    "\n",
    "monthly_aggregated_df = merged_df.groupby(['country', 'year_month'])[aggregated_columns].sum().reset_index()\n",
    "# Group the DataFrame by 'country' and 'year_month', sums the specified columns, and resets the index\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "print(monthly_aggregated_df)\n",
    "# Print the aggregated DataFrame\n",
    "\n",
    "# Save the aggregated DataFrame to a CSV file\n",
    "monthly_aggregated_df.to_csv(os.path.join(directory_path, 'monthly_aggregated_country.csv'), index=False)\n",
    "# Save the aggregated DataFrame to a new CSV file in the specified directory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
